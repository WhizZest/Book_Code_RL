{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "# Q网络定义\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# 经验回放池\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (\n",
    "            torch.tensor(np.stack(states), dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.int64),\n",
    "            torch.tensor(rewards, dtype=torch.float32),\n",
    "            torch.tensor(np.stack(next_states), dtype=torch.float32),\n",
    "            torch.tensor(dones, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_dataList(rewards, xStart=0):\n",
    "    # 设置图像的宽度为 12 英寸\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(range(xStart, len(rewards) + xStart), rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Total Reward vs Episode')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.V = nn.Linear(128, 1)\n",
    "        self.A = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        Q = V + (A - A.mean(dim=1, keepdim=True))\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        \"\"\"\n",
    "        capacity: 缓冲区容量\n",
    "        alpha: 优先级比例，0表示完全随机采样，1表示完全按优先级采样\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"添加新经验，优先级初始化为最大值以确保被采样\"\"\"\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.priorities[self.position] = max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        \"\"\"\n",
    "        采样带优先级的批次数据\n",
    "        beta: 重要性采样的偏置修正参数\n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[:self.position]\n",
    "        \n",
    "        # 根据优先级分布计算采样概率\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        # 按照概率采样\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        # 计算重要性采样权重\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        # 解包样本\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        return (\n",
    "            torch.tensor(np.stack(states), dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.int64),\n",
    "            torch.tensor(rewards, dtype=torch.float32),\n",
    "            torch.tensor(np.stack(next_states), dtype=torch.float32),\n",
    "            torch.tensor(dones, dtype=torch.float32),\n",
    "            torch.tensor(weights, dtype=torch.float32),\n",
    "            indices,\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"根据新的 TD Error 更新优先级\"\"\"\n",
    "        self.priorities[indices] = priorities\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std_init=0.5):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "\n",
    "        # 可训练参数\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "\n",
    "        # 非参数化噪声\n",
    "        self.register_buffer(\"weight_epsilon\", torch.empty(out_features, in_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.empty(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # 初始化可训练参数\n",
    "        bound = 1 / self.in_features ** 0.5\n",
    "        self.weight_mu.data.uniform_(-bound, bound)\n",
    "        self.weight_sigma.data.fill_(self.std_init / (self.in_features ** 0.5))\n",
    "        self.bias_mu.data.uniform_(-bound, bound)\n",
    "        self.bias_sigma.data.fill_(self.std_init / (self.in_features ** 0.5))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        # 采样噪声\n",
    "        self.weight_epsilon.normal_()\n",
    "        self.bias_epsilon.normal_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return torch.nn.functional.linear(x, weight, bias)\n",
    "\n",
    "class Dueling_NoisyDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Dueling_NoisyDQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            NoisyLinear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(128, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.V = NoisyLinear(128, 1)\n",
    "        self.A = NoisyLinear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        Q = V + (A - A.mean(dim=1, keepdim=True))\n",
    "        return Q\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, NoisyLinear):\n",
    "                m.reset_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepPrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, n_step=3, gamma=0.99):\n",
    "        \"\"\"\n",
    "        capacity: 缓冲区容量\n",
    "        alpha: 优先级比例，0表示完全随机采样，1表示完全按优先级采样\n",
    "        n_step: 多步时间跨度\n",
    "        gamma: 折扣因子\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.alpha = alpha\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # 用于多步存储的临时队列\n",
    "        self.n_step_queue = []\n",
    "\n",
    "    def _get_n_step_info(self):\n",
    "        \"\"\"从 n_step_queue 计算 n 步累计奖励和目标状态\"\"\"\n",
    "        R = 0\n",
    "        # 实际队列长度可能小于 n_step\n",
    "        n_step = len(self.n_step_queue)\n",
    "        for idx, (_, _, reward, _, _) in enumerate(self.n_step_queue):\n",
    "            R += (self.gamma ** idx) * reward\n",
    "        state, action, _, next_state, done = self.n_step_queue[0]\n",
    "        final_next_state, final_done = self.n_step_queue[-1][3], self.n_step_queue[-1][4]\n",
    "        return (state, action, R, final_next_state, final_done, n_step)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        添加新经验。\n",
    "        使用 n_step_queue 缓存多步数据，只有在积累到 n 步时才存入 buffer。\n",
    "        在轨迹结束时处理剩余的队列。\n",
    "        \"\"\"\n",
    "        self.n_step_queue.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # 如果 n_step_queue 满了，处理一个完整的 n-step 转移\n",
    "        if len(self.n_step_queue) == self.n_step:\n",
    "            n_step_transition = self._get_n_step_info()\n",
    "            max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append(n_step_transition)\n",
    "            else:\n",
    "                self.buffer[self.position] = n_step_transition\n",
    "            self.priorities[self.position] = max_priority\n",
    "            self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "            # 移除队列的第一个元素\n",
    "            self.n_step_queue.pop(0)\n",
    "\n",
    "        # 如果 done=True，处理剩余队列中的短步转移\n",
    "        if done:\n",
    "            while self.n_step_queue:\n",
    "                n_step_transition = self._get_n_step_info()\n",
    "                max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "                if len(self.buffer) < self.capacity:\n",
    "                    self.buffer.append(n_step_transition)\n",
    "                else:\n",
    "                    self.buffer[self.position] = n_step_transition\n",
    "                self.priorities[self.position] = max_priority\n",
    "                self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "                # 移除队列的第一个元素\n",
    "                self.n_step_queue.pop(0)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4, device='cpu'):\n",
    "        \"\"\"\n",
    "        采样带优先级的批次数据。\n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[:self.position]\n",
    "\n",
    "        # 根据优先级分布计算采样概率\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        # 按照概率采样\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        # 计算重要性采样权重\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        # 解包样本\n",
    "        states, actions, rewards, next_states, dones, n_steps = zip(*samples)\n",
    "        return (\n",
    "            torch.tensor(np.stack(states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(actions, dtype=torch.int64).to(device),\n",
    "            torch.tensor(rewards, dtype=torch.float32).to(device),\n",
    "            torch.tensor(np.stack(next_states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(dones, dtype=torch.float32).to(device),\n",
    "            torch.tensor(weights, dtype=torch.float32).to(device),\n",
    "            torch.tensor(n_steps, dtype=torch.int64).to(device),\n",
    "            indices,\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"根据新的 TD Error 更新优先级\"\"\"\n",
    "        self.priorities[indices] = priorities\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def current_queue_size(self):\n",
    "        return len(self.n_step_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.3 (SDL 2.0.16, Python 3.8.20)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pygame\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import configparser\n",
    "\n",
    "fileName = 'models/fb_v0_no_score_2024-12-10_21-45-31.pth'\n",
    "bestScoreFileName = 'flappy_bird_v0_model_best_score.pth'\n",
    "stopTrainingFileName = 'flappy_bird_stop.txt'\n",
    "guideFileName = 'result1/models/fb_v0_no_score_2024-12-07_16-37-46.pth'\n",
    "\n",
    "def train_dueling_dqn_noise_MultiStep_PER(env, num_episodes=500, batch_size=64, gamma=0.99, \n",
    "                                          epsilon_schedule=[(0, 1.0), (20000, 0.1), (700000, 0.01), (1040000, 0.001), (1720000, 0.0001), (2060000, 0.0)], \n",
    "                                          lr=1e-3, alpha=0.6, beta_start=0.4, beta_increment=1e-4):\n",
    "    # 新的状态维度为原始状态维度的 8 倍\n",
    "    number_of_states = 12\n",
    "    input_dim = env.observation_space.shape[0] * number_of_states\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    # 检查是否有GPU可用\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    q_net = Dueling_NoisyDQN(input_dim, output_dim).to(device)\n",
    "    # 判断是否存在fileName文件\n",
    "    if os.path.exists(fileName):\n",
    "        q_net.load_state_dict(torch.load(fileName, weights_only=True, map_location=device))\n",
    "        print(\"模型已加载\")\n",
    "    q_net.train() # 设置为训练模式，需要通过训练更新参数，该行代码可以省略，因为默认就是训练模式\n",
    "    target_net = Dueling_NoisyDQN(input_dim, output_dim).to(device)\n",
    "    target_net.load_state_dict(q_net.state_dict())\n",
    "    target_net.eval() # 设置为评估模式，不需要通过训练更新参数，更新时只需要复制q_net的参数\n",
    "    guider_net = Dueling_NoisyDQN(env.observation_space.shape[0], output_dim)\n",
    "    guideOpen = False\n",
    "    if os.path.exists(guideFileName):\n",
    "        guider_net.load_state_dict(torch.load(guideFileName, weights_only=True))\n",
    "        guider_net = guider_net.to(device)\n",
    "        guider_net.eval()\n",
    "        print(\"引导模型已加载\")\n",
    "    else:\n",
    "        guider_net = None\n",
    "\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
    "    replay_buffer_capacity = 100000\n",
    "    replay_buffer = MultiStepPrioritizedReplayBuffer(capacity=replay_buffer_capacity, alpha=alpha, n_step=20, gamma=gamma)\n",
    "    epsilon = epsilon_schedule[0][1]\n",
    "    beta = beta_start\n",
    "    rewards = []  # 确保它是一个列表\n",
    "    max_reward_total = -np.inf\n",
    "    max_interval_rewards = -np.inf\n",
    "    min_interval_rewards = np.inf\n",
    "    max_score = 0\n",
    "    max_step_count = 0\n",
    "    update_step_count = 0\n",
    "    update_step_interval = 200\n",
    "    print_interval = 300  # 间隔（单位：秒）\n",
    "    # 记录训练开始的时间\n",
    "    last_save_time = time.time()\n",
    "    last_print_time = last_save_time\n",
    "    stop_training = False\n",
    "    steps_Interval = 1000\n",
    "    steps_perInterval = 0\n",
    "    steps_total = 0\n",
    "    loss_perInterval = 0\n",
    "    q_value_perInterval = 0\n",
    "    delta_training_frequency = 3\n",
    "    delta_loss_threshold = 0.1\n",
    "    # 获取当前时间戳\n",
    "    current_time_str = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    csv_file = f'dueling_dqn_noise_MultiStep_PER_{current_time_str}.csv'\n",
    "    aim_score = 5000\n",
    "\n",
    "    # 创建表格文件，列名分别为：总步数、epsilon、平均损失、平均Q值\n",
    "    with open(csv_file, 'w') as f:\n",
    "        f.write('Time,episode,Steps,epsilon,loss,Q_value\\n')\n",
    "        f.close()\n",
    "    ratio_schedule = []\n",
    "    for i in range(len(epsilon_schedule) - 1):\n",
    "        start_step, start_epsilon = epsilon_schedule[i]\n",
    "        end_step, end_epsilon = epsilon_schedule[i + 1]\n",
    "        ratio = (end_epsilon - start_epsilon) / (end_step - start_step)\n",
    "        ratio_schedule.append(ratio)\n",
    "    \n",
    "    print(f\"Episode | min interval reward | max interval reward | max_reward_total | Epsilon | max_score | steps_total | lr | update_step_interval | training_frequency | loss_threshold | beta | replay_buffer.size\")\n",
    "    for episode in range(num_episodes):\n",
    "        raw_state = env.reset()  # 返回 numpy.ndarray\n",
    "        state_queue = deque([raw_state.copy() for _ in range(number_of_states)], maxlen=number_of_states)  # 初始化队列，初始状态填充队列\n",
    "        state = np.concatenate(state_queue) # 将队列内容展平\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        score = 0\n",
    "\n",
    "        while not done:  # 每个 episode 的最大步数\n",
    "            # 根据多段线性衰减策略计算 epsilon\n",
    "            for i in range(len(epsilon_schedule) - 1):\n",
    "                start_step, start_epsilon = epsilon_schedule[i]\n",
    "                end_step, end_epsilon = epsilon_schedule[i + 1]\n",
    "                if start_step <= steps_total < end_step:\n",
    "                    # 在当前阶段内进行线性插值\n",
    "                    ratio = ratio_schedule[i]\n",
    "                    epsilon = max(start_epsilon + ratio * (steps_total - start_step), end_epsilon)\n",
    "            # ε-贪婪策略\n",
    "            if random.random() < epsilon:\n",
    "                # 根据概率决定采样\n",
    "                if random.random() < 0.08:\n",
    "                    action = 1\n",
    "                else:\n",
    "                    action = 0\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    if guideOpen and guider_net is not None:\n",
    "                        action = guider_net(torch.tensor(raw_state, dtype=torch.float32).unsqueeze(0).to(device)).argmax().item()\n",
    "                    else:\n",
    "                        action = q_net(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)).argmax().item()\n",
    "\n",
    "            # 执行动作\n",
    "            next_raw_state, reward, done, info = env.step(action)\n",
    "            step_count += 1\n",
    "            steps_total += 1\n",
    "            if max_step_count < step_count:\n",
    "                max_step_count = step_count\n",
    "            raw_state = next_raw_state\n",
    "            # 更新状态队列\n",
    "            state_queue.append(next_raw_state)\n",
    "            next_state = np.concatenate(state_queue)  # 将队列内容展平\n",
    "            reward = reward * 0.01  # 缩放奖励\n",
    "            # 得分\n",
    "            bScore = False\n",
    "            if info['score'] > score:\n",
    "                reward += 0.04  # 奖励增加\n",
    "                score = info['score']\n",
    "                bScore = True\n",
    "            if info['score'] > max_score:\n",
    "                max_score = info['score']\n",
    "                if max_score > 100:\n",
    "                    torch.save(q_net.state_dict(), bestScoreFileName) # 保存模型\n",
    "                env.render()\n",
    "                time.sleep(1 / 60)  # FPS\n",
    "                for event in pygame.event.get():\n",
    "                    pass\n",
    "            if done:\n",
    "                reward -= 0.02  # 惩罚\n",
    "            '''if bScore:\n",
    "                if score % aim_score == 0:\n",
    "                    replay_buffer.add(state, action, reward, next_state, True) # 得到aim_score分结束，否则轨迹越长，Q值越大，无法收敛\n",
    "                else:\n",
    "                    replay_buffer.add(state, action, reward, next_state, done)\n",
    "            else:'''\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if total_reward > max_reward_total:\n",
    "                max_reward_total = total_reward\n",
    "            if max_interval_rewards < total_reward:\n",
    "                max_interval_rewards = total_reward\n",
    "\n",
    "            # 经验回放训练\n",
    "            if replay_buffer.size() >= batch_size:\n",
    "                training_frequency = 1\n",
    "                loss_threshold = 0.5\n",
    "                while training_frequency > 0:\n",
    "                    # 从优先级缓冲区中采样\n",
    "                    states, actions, rewards_batch, next_states, dones, weights, n_step_batch, indices = replay_buffer.sample(batch_size, beta, device)\n",
    "\n",
    "                    q_values = q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                    with torch.no_grad():\n",
    "                        q_value_perInterval += q_values.mean().item() / steps_Interval\n",
    "                        best_actions = q_net(next_states).argmax(1)  # 使用当前网络选择最大Q值的动作\n",
    "                        target_q_values = rewards_batch + (gamma ** n_step_batch) * (1 - dones) * target_net(next_states).gather(1, best_actions.unsqueeze(1)).squeeze(1)\n",
    "                    # 计算 TD Error\n",
    "                    td_errors = target_q_values - q_values\n",
    "                    loss = (weights * td_errors.pow(2)).mean()\n",
    "                    loss_perInterval += loss.item() / steps_Interval\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # 更新优先级\n",
    "                    priorities = td_errors.abs().detach().cpu().numpy()\n",
    "                    replay_buffer.update_priorities(indices, priorities)\n",
    "                    update_step_count += 1\n",
    "                    steps_perInterval += 1\n",
    "                \n",
    "                    # 更新目标网络\n",
    "                    if update_step_count >= update_step_interval:\n",
    "                        update_step_count = 0\n",
    "                        target_net.load_state_dict(q_net.state_dict()) # 将q_net的参数复制到target_net中\n",
    "                    training_frequency -= 1\n",
    "                    if loss.item() > loss_threshold and replay_buffer.size() >= replay_buffer_capacity * 0.5 and training_frequency == 0: # 如果损失大于阈值，且经验池已足够大，则重复训练，暂停与环境互动\n",
    "                        training_frequency += delta_training_frequency\n",
    "                        loss_threshold += delta_loss_threshold\n",
    "                    if steps_perInterval >= steps_Interval:\n",
    "                        # 追加数据到 CSV 文件\n",
    "                        with open(csv_file, 'a') as f:\n",
    "                            current_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                            f.write(f'{current_time_str},{episode},{steps_total},{epsilon},{loss_perInterval},{q_value_perInterval}\\n')\n",
    "                        steps_perInterval = 0\n",
    "                        loss_perInterval = 0\n",
    "                        q_value_perInterval = 0\n",
    "                    # 检查时间间隔\n",
    "                    current_time = time.time()\n",
    "                    if current_time - last_save_time > 60:\n",
    "                        # 读取配置文件\n",
    "                        config = configparser.ConfigParser()\n",
    "                        if config.read('config.ini'):\n",
    "                            update_step_interval = config.getint('Training', 'update_step_interval')\n",
    "                            delta_training_frequency_temp = config.getint('Training', 'delta_training_frequency')\n",
    "                            if delta_training_frequency_temp >= 0:\n",
    "                                delta_training_frequency = delta_training_frequency_temp\n",
    "                            delta_loss_threshold_temp = config.getfloat('Training', 'delta_loss_threshold')\n",
    "                            if delta_loss_threshold_temp > 0:\n",
    "                                delta_loss_threshold = delta_loss_threshold_temp\n",
    "                            lr_temp = config.getfloat('Training', 'lr')\n",
    "                            if lr_temp > 0:\n",
    "                                lr = lr_temp\n",
    "                                state_dict = optimizer.state_dict()\n",
    "                                state_dict['param_groups'][0]['lr'] = lr\n",
    "                                optimizer.load_state_dict(state_dict)\n",
    "                            beta_temp = config.getfloat('Training', 'beta')\n",
    "                            if beta_temp > 0:\n",
    "                                beta = beta_temp\n",
    "                            stop_training = config.getboolean('Training', 'stop_training')\n",
    "                            guideOpen = config.getboolean('Training', 'guideOpen')\n",
    "                            \n",
    "                        # 设置保存路径和合法文件名\n",
    "                        save_path = \"./models\"\n",
    "                        os.makedirs(save_path, exist_ok=True)  # 确保路径存在\n",
    "                        current_time_str = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')  # 使用合法字符\n",
    "                        currentNetFile = os.path.join(save_path, f'fb_v0_no_score_{current_time_str}.pth')\n",
    "                        torch.save(q_net.state_dict(), currentNetFile) # 保存模型\n",
    "                        last_save_time = current_time\n",
    "                    if current_time - last_print_time >= print_interval:\n",
    "                        last_print_time = current_time\n",
    "                        print(f\"{episode} | {min_interval_rewards:.3f} | {max_interval_rewards:.3f} | {max_reward_total:.3f} | {epsilon:.5f} | {max_score} | {steps_total} | {lr:.8e} | {update_step_interval} | {training_frequency} | {loss_threshold:.3f} | {beta:.5f} | {replay_buffer.size()}\")\n",
    "                        min_interval_rewards = np.inf\n",
    "                        max_interval_rewards = -np.inf\n",
    "                    # 更新 beta\n",
    "                    beta = min(1.0, beta + beta_increment)\n",
    "\n",
    "        rewards.append(total_reward)  # 确保 append 正常工作\n",
    "        if min_interval_rewards > total_reward:\n",
    "            min_interval_rewards = total_reward\n",
    "        # 重置噪声\n",
    "        q_net.reset_noise()\n",
    "        target_net.reset_noise()\n",
    "        if stop_training:\n",
    "            # 把配置文件的stop_training改为False\n",
    "            config['Training']['stop_training'] = 'False'\n",
    "            with open('config.ini', 'w') as configfile:\n",
    "                config.write(configfile)\n",
    "            break\n",
    "\n",
    "    return q_net, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "class Dueling_DistributionalDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_atoms=51):\n",
    "        super(Dueling_DistributionalDQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            NoisyLinear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(128, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.V = NoisyLinear(128, num_atoms)\n",
    "        self.A = NoisyLinear(128, output_dim * num_atoms)\n",
    "        self.num_atoms = num_atoms\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        V = self.V(x).view(-1, 1, self.num_atoms)\n",
    "        A = self.A(x).view(-1, self.output_dim, self.num_atoms)\n",
    "        Q = V + (A - A.mean(dim=1, keepdim=True))\n",
    "        Q_prob = F.softmax(Q, dim=2) # 将 Q 值转换为概率分布\n",
    "        return Q_prob\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, NoisyLinear):\n",
    "                m.reset_noise()\n",
    "\n",
    "def projection_distribution(next_dist, rewards, dones, gamma, atoms, v_min, v_max, delta_z, support):\n",
    "    \"\"\"\n",
    "    投影 Bellman 更新后的分布到支持点。\n",
    "    \"\"\"\n",
    "    #delta_z = (v_max - v_min) / (atoms - 1)\n",
    "    #support = torch.linspace(v_min, v_max, atoms).to(next_dist.device)  # Shape: (atoms,)\n",
    "    \n",
    "    batch_size = rewards.size(0)\n",
    "    next_support = rewards.unsqueeze(1) + gamma * support.unsqueeze(0) * (1 - dones.unsqueeze(1))  # Shape: (batch_size, atoms)\n",
    "    next_support = next_support.clamp(v_min, v_max)  # 限制范围\n",
    "\n",
    "    b = (next_support - v_min) / delta_z  # Shape: (batch_size, atoms)\n",
    "    l = b.floor().long()  # Shape: (batch_size, atoms)\n",
    "    u = b.ceil().long()  # Shape: (batch_size, atoms)\n",
    "    \n",
    "    # 修正索引的范围，确保不越界\n",
    "    l = l.clamp(0, atoms - 1)\n",
    "    u = u.clamp(0, atoms - 1)\n",
    "    \n",
    "    proj_dist = torch.zeros(batch_size, atoms).to(next_dist.device)  # Shape: (batch_size, atoms)\n",
    "\n",
    "    for i in range(atoms):  # 遍历每个支持点\n",
    "        # 注意：next_dist[:, i] 实际是 batch_size 的第 i 列 (shape: [batch_size])\n",
    "        # next_dist 应被广播以匹配 l 和 u 的维度\n",
    "        weight_left = (u[:, i] - b[:, i]).unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "        weight_right = (b[:, i] - l[:, i]).unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "        \n",
    "        proj_dist.scatter_add_(1, l[:, i].unsqueeze(1), next_dist[:, i].unsqueeze(1) * weight_left)\n",
    "        proj_dist.scatter_add_(1, u[:, i].unsqueeze(1), next_dist[:, i].unsqueeze(1) * weight_right)\n",
    "\n",
    "    # 归一化分布\n",
    "    proj_dist /= proj_dist.sum(dim=1, keepdim=True) + 1e-8  # 防止除零\n",
    "    return proj_dist\n",
    "\n",
    "def train_rainbow_dqn(env, num_episodes=500, batch_size=64, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01,\n",
    "                      epsilon_decay=500, lr=1e-3, alpha=0.6, beta_start=0.4, beta_increment=1e-4,\n",
    "                      atoms=51, v_min=-10, v_max=10):\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    q_net = Dueling_DistributionalDQN(input_dim, output_dim, num_atoms=atoms)\n",
    "    target_net = Dueling_DistributionalDQN(input_dim, output_dim, num_atoms=atoms)\n",
    "    target_net.load_state_dict(q_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
    "    replay_buffer = MultiStepPrioritizedReplayBuffer(capacity=10000, alpha=alpha, n_step=3, gamma=gamma)\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "    beta = beta_start\n",
    "    delta_z = (v_max - v_min) / (atoms - 1)\n",
    "    rewards = []\n",
    "    supports = torch.linspace(v_min, v_max, atoms)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(2000):  # 每个 episode 的最大步数\n",
    "            # ε-贪婪策略\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    dist = q_net(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "                    action = (dist * supports).sum(dim=2).argmax().item()\n",
    "\n",
    "            # 执行动作\n",
    "            # 渲染环境\n",
    "            #env.render()\n",
    "            #time.sleep(1 / 30)  # FPS\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if reward < 0:\n",
    "                print(\"reward:\", reward)\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # 经验回放训练\n",
    "            if replay_buffer.size() >= batch_size:\n",
    "                # 从优先级缓冲区中采样\n",
    "                states, actions, rewards_batch, next_states, dones, weights, indices = replay_buffer.sample(batch_size, beta)\n",
    "\n",
    "                # 计算 Q 网络的分布\n",
    "                dist = q_net(states)\n",
    "                q_dist = dist[range(batch_size), actions]\n",
    "\n",
    "                # 目标分布计算\n",
    "                with torch.no_grad():\n",
    "                    # 目标网络输出分布\n",
    "                    next_dist = target_net(next_states)  # Shape: (batch_size, num_actions, atoms)\n",
    "                    # 行为网络选择动作（Double-DQN）\n",
    "                    next_q_values = (q_net(next_states) * supports.to(next_states.device)).sum(dim=2)  # Shape: (batch_size, num_actions)\n",
    "                    next_actions = next_q_values.argmax(dim=1)  # Shape: (batch_size,)\n",
    "                    # 根据行为网络选择的动作提取目标分布\n",
    "                    next_dist = next_dist[range(batch_size), next_actions]  # Shape: (batch_size, atoms)\n",
    "\n",
    "                    # 投影分布\n",
    "                    target_dist = projection_distribution(next_dist, rewards_batch, dones, gamma, atoms, v_min, v_max, delta_z, supports.to(next_states.device))\n",
    "\n",
    "                # KL 散度损失\n",
    "                loss = -(target_dist * q_dist.log()).sum(dim=1) * weights\n",
    "                loss = loss.mean()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # 更新优先级\n",
    "                # Wasserstein 距离计算\n",
    "                td_errors = torch.sum((target_dist - q_dist) * supports.to(next_states.device), dim=1)  # [batch_size]\n",
    "                priorities = td_errors.abs().detach().cpu().numpy()\n",
    "                replay_buffer.update_priorities(indices, priorities)\n",
    "\n",
    "        # 更新目标网络\n",
    "        if episode % 10 == 0:\n",
    "            target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "        # 更新 epsilon\n",
    "        epsilon = max(epsilon_end, epsilon_start - episode / epsilon_decay)\n",
    "        rewards.append(total_reward)\n",
    "        # 更新 beta\n",
    "        beta = min(1.0, beta + beta_increment)\n",
    "        # 重置噪声\n",
    "        q_net.reset_noise()\n",
    "        target_net.reset_noise()\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    return q_net, rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Episode | min interval reward | max interval reward | max_reward_total | Epsilon | max_score | steps_total | lr | update_step_interval | training_frequency | loss_threshold | beta | replay_buffer.size\n",
      "606 | 0.300 | 1.460 | 1.460 | 0.95535 | 1 | 54511 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 54492\n",
      "1058 | 0.300 | 1.560 | 1.560 | 0.51483 | 2 | 99008 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 98989\n",
      "1529 | 0.300 | 1.810 | 1.810 | 0.05882 | 2 | 145070 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "1972 | 0.300 | 1.600 | 1.810 | 0.01000 | 2 | 190693 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "2386 | 0.300 | 2.630 | 2.630 | 0.01000 | 4 | 236884 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "2723 | 0.300 | 4.460 | 4.460 | 0.00704 | 9 | 282893 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "2976 | 0.300 | 8.380 | 8.380 | 0.00288 | 18 | 329098 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3138 | 0.300 | 17.400 | 17.400 | 0.00100 | 40 | 375431 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3238 | 0.300 | 31.730 | 31.730 | 0.00100 | 75 | 421899 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3329 | 0.300 | 25.990 | 31.730 | 0.00067 | 75 | 468400 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3403 | 0.490 | 59.210 | 59.210 | 0.00009 | 142 | 514607 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3428 | 0.490 | 63.560 | 63.560 | 0.00006 | 153 | 560694 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3475 | 0.300 | 54.120 | 63.560 | 0.00003 | 153 | 606286 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3496 | 0.490 | 145.560 | 145.560 | 0.00000 | 353 | 646642 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3523 | 0.300 | 106.350 | 145.560 | 0.00000 | 353 | 692572 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3556 | 1.220 | 60.850 | 145.560 | 0.00000 | 353 | 738933 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3586 | 0.300 | 102.250 | 145.560 | 0.00000 | 353 | 784804 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3608 | 1.390 | 115.790 | 145.560 | 0.00000 | 353 | 830914 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3640 | 2.200 | 47.730 | 145.560 | 0.00000 | 353 | 877062 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3657 | 1.390 | 109.240 | 145.560 | 0.00000 | 353 | 923325 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3692 | 0.300 | 134.650 | 145.560 | 0.00000 | 353 | 969771 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3720 | 0.990 | 56.750 | 145.560 | 0.00000 | 353 | 1015836 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3745 | 1.390 | 71.090 | 145.560 | 0.00000 | 353 | 1062003 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3771 | 0.300 | 84.190 | 145.560 | 0.00000 | 353 | 1107727 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3789 | 3.020 | 140.390 | 145.560 | 0.00000 | 353 | 1153796 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3804 | 1.000 | 80.110 | 145.560 | 0.00000 | 353 | 1199231 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3813 | 7.120 | 134.230 | 145.560 | 0.00000 | 353 | 1244963 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3828 | 1.390 | 148.590 | 148.590 | 0.00000 | 360 | 1290725 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3847 | 0.490 | 120.710 | 148.590 | 0.00000 | 360 | 1336345 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3863 | 1.390 | 157.190 | 157.190 | 0.00000 | 381 | 1382092 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3870 | 21.060 | 166.210 | 166.210 | 0.00000 | 403 | 1428044 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3887 | 0.300 | 99.790 | 166.210 | 0.00000 | 403 | 1473954 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3897 | 1.390 | 117.830 | 166.210 | 0.00000 | 403 | 1520088 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3906 | 1.390 | 293.730 | 293.730 | 0.00000 | 714 | 1564007 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3923 | 0.300 | 117.830 | 293.730 | 0.00000 | 714 | 1609739 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3939 | 1.800 | 94.870 | 293.730 | 0.00000 | 714 | 1655527 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3964 | 0.490 | 74.370 | 293.730 | 0.00000 | 714 | 1701577 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3981 | 0.990 | 101.850 | 293.730 | 0.00000 | 714 | 1747470 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3996 | 1.390 | 86.670 | 293.730 | 0.00000 | 714 | 1793175 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4010 | 2.200 | 112.910 | 293.730 | 0.00000 | 714 | 1839502 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4020 | 0.300 | 159.250 | 293.730 | 0.00000 | 714 | 1886039 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4032 | 1.980 | 114.660 | 293.730 | 0.00000 | 714 | 1932529 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4048 | 1.390 | 128.090 | 293.730 | 0.00000 | 714 | 1978700 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4065 | 0.490 | 119.470 | 293.730 | 0.00000 | 714 | 2025164 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4072 | 3.690 | 307.250 | 307.250 | 0.00000 | 747 | 2071868 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4090 | 1.390 | 93.650 | 307.250 | 0.00000 | 747 | 2118754 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4097 | 14.100 | 255.590 | 307.250 | 0.00000 | 747 | 2165670 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4110 | 15.740 | 126.560 | 307.250 | 0.00000 | 747 | 2212388 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4124 | 1.060 | 117.430 | 307.250 | 0.00000 | 747 | 2258804 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4140 | 2.200 | 147.460 | 307.250 | 0.00000 | 747 | 2305015 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4152 | 0.300 | 92.410 | 307.250 | 0.00000 | 747 | 2351591 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4164 | 1.880 | 157.440 | 307.250 | 0.00000 | 747 | 2398235 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4186 | 1.220 | 93.230 | 307.250 | 0.00000 | 747 | 2445163 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4195 | 4.260 | 137.510 | 307.250 | 0.00000 | 747 | 2491735 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4208 | 1.800 | 89.950 | 307.250 | 0.00000 | 747 | 2538201 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4217 | 5.080 | 268.710 | 307.250 | 0.00000 | 747 | 2584640 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4226 | 0.300 | 208.850 | 307.250 | 0.00000 | 747 | 2630003 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4228 | 139.960 | 372.030 | 372.030 | 0.00000 | 905 | 2672145 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4231 | 18.600 | 345.390 | 372.030 | 0.00000 | 905 | 2715900 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4242 | 1.390 | 210.910 | 372.030 | 0.00000 | 905 | 2759844 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4253 | 1.800 | 209.670 | 372.030 | 0.00000 | 905 | 2804483 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4265 | 1.800 | 93.650 | 372.030 | 0.00000 | 905 | 2849164 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4275 | 8.760 | 107.170 | 372.030 | 0.00000 | 905 | 2893686 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4281 | 10.650 | 271.990 | 372.030 | 0.00000 | 905 | 2938178 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4286 | 3.440 | 221.970 | 372.030 | 0.00000 | 905 | 2981231 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4290 | 21.480 | 208.850 | 372.030 | 0.00000 | 905 | 3024627 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4297 | 2.620 | 216.230 | 372.030 | 0.00000 | 905 | 3068102 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4310 | 0.300 | 234.690 | 372.030 | 0.00000 | 905 | 3110963 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4314 | 0.990 | 440.910 | 440.910 | 0.00000 | 1073 | 3153175 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4319 | 2.860 | 194.340 | 440.910 | 0.00000 | 1073 | 3196710 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4319 | inf | 555.260 | 555.260 | 0.00000 | 1352 | 3238099 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4321 | 179.320 | 556.950 | 556.950 | 0.00000 | 1356 | 3281057 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4326 | 39.920 | 360.150 | 556.950 | 0.00000 | 1356 | 3324156 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4329 | 58.780 | 236.730 | 556.950 | 0.00000 | 1356 | 3366935 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4329 | inf | 591.410 | 591.410 | 0.00000 | 1440 | 3410059 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4337 | 0.300 | 759.490 | 759.490 | 0.00000 | 1850 | 3451418 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4337 | inf | 686.580 | 759.490 | 0.00000 | 1850 | 3494914 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4342 | 1.390 | 731.190 | 759.490 | 0.00000 | 1850 | 3537845 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4348 | 9.580 | 108.660 | 759.490 | 0.00000 | 1850 | 3579935 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4353 | 4.890 | 207.210 | 759.490 | 0.00000 | 1850 | 3622332 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4356 | 9.010 | 283.330 | 759.490 | 0.00000 | 1850 | 3664476 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4357 | 691.820 | 691.830 | 759.490 | 0.00000 | 1850 | 3707166 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4363 | 1.390 | 349.890 | 759.490 | 0.00000 | 1850 | 3750161 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4365 | 150.050 | 399.910 | 759.490 | 0.00000 | 1850 | 3792942 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4370 | 0.300 | 215.410 | 759.490 | 0.00000 | 1850 | 3836050 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4371 | 290.020 | 290.030 | 759.490 | 0.00000 | 1850 | 3879815 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4371 | inf | 767.340 | 767.340 | 0.00000 | 1869 | 3923708 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4371 | inf | 1211.900 | 1211.900 | 0.00000 | 2954 | 3963824 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4371 | inf | 1674.570 | 1674.570 | 0.00000 | 4082 | 4005579 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4371 | inf | 2136.510 | 2136.510 | 0.00000 | 5209 | 4047265 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4371 | inf | 2566.990 | 2566.990 | 0.00000 | 6259 | 4086113 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4371 | inf | 3031.220 | 3031.220 | 0.00000 | 7391 | 4128008 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4372 | 3462.200 | 3462.210 | 3462.210 | 0.00000 | 8442 | 4168764 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4374 | 0.990 | 268.710 | 3462.210 | 0.00000 | 8442 | 4211607 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4376 | 208.840 | 340.050 | 3462.210 | 0.00000 | 8442 | 4255343 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4379 | 67.400 | 319.970 | 3462.210 | 0.00000 | 8442 | 4299444 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4380 | 479.440 | 479.450 | 3462.210 | 0.00000 | 8442 | 4343998 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4381 | 379.400 | 379.410 | 3462.210 | 0.00000 | 8442 | 4386495 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4381 | inf | 687.210 | 3462.210 | 0.00000 | 8442 | 4425480 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4383 | 223.200 | 789.010 | 3462.210 | 0.00000 | 8442 | 4464877 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4386 | 17.780 | 312.990 | 3462.210 | 0.00000 | 8442 | 4507639 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4388 | 214.580 | 373.270 | 3462.210 | 0.00000 | 8442 | 4551098 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4390 | 1.390 | 253.040 | 3462.210 | 0.00000 | 8442 | 4595563 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4396 | 0.300 | 290.950 | 3462.210 | 0.00000 | 8442 | 4640650 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4402 | 0.990 | 297.410 | 3462.210 | 0.00000 | 8442 | 4685687 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4405 | 25.980 | 318.730 | 3462.210 | 0.00000 | 8442 | 4730088 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4408 | 135.860 | 281.010 | 3462.210 | 0.00000 | 8442 | 4773281 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4408 | inf | 484.000 | 3462.210 | 0.00000 | 8442 | 4816328 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4409 | 640.160 | 640.170 | 3462.210 | 0.00000 | 8442 | 4859249 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4413 | 1.800 | 697.570 | 3462.210 | 0.00000 | 8442 | 4902432 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4418 | 0.990 | 386.390 | 3462.210 | 0.00000 | 8442 | 4945826 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4428 | 0.300 | 229.880 | 3462.210 | 0.00000 | 8442 | 4989932 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4429 | 421.220 | 421.230 | 3462.210 | 0.00000 | 8442 | 5033897 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4431 | 326.520 | 349.890 | 3462.210 | 0.00000 | 8442 | 5077626 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4431 | inf | 591.430 | 3462.210 | 0.00000 | 8442 | 5121647 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4431 | inf | 1085.520 | 3462.210 | 0.00000 | 8442 | 5166236 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4433 | 101.420 | 1264.190 | 3462.210 | 0.00000 | 8442 | 5210804 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4433 | inf | 709.400 | 3462.210 | 0.00000 | 8442 | 5255552 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4435 | 20.680 | 1083.790 | 3462.210 | 0.00000 | 8442 | 5299006 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4436 | 503.220 | 503.230 | 3462.210 | 0.00000 | 8442 | 5342616 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4441 | 0.490 | 392.980 | 3462.210 | 0.00000 | 8442 | 5386336 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4449 | 0.300 | 156.370 | 3462.210 | 0.00000 | 8442 | 5429943 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4461 | 1.390 | 230.170 | 3462.210 | 0.00000 | 8442 | 5474037 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4468 | 1.390 | 359.730 | 3462.210 | 0.00000 | 8442 | 5518229 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4473 | 5.910 | 283.890 | 3462.210 | 0.00000 | 8442 | 5575182 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4483 | 0.300 | 367.930 | 3462.210 | 0.00000 | 8442 | 5638504 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4489 | 1.390 | 454.030 | 3462.210 | 0.00000 | 8442 | 5701688 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4495 | 0.300 | 387.670 | 3462.210 | 0.00000 | 8442 | 5764590 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4516 | 0.990 | 554.070 | 3462.210 | 0.00000 | 8442 | 5827070 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4528 | 0.300 | 450.750 | 3462.210 | 0.00000 | 8442 | 5889963 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4538 | 0.990 | 369.170 | 3462.210 | 0.00000 | 8442 | 5952111 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4542 | 5.480 | 292.490 | 3462.210 | 0.00000 | 8442 | 6015219 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4547 | 3.840 | 359.730 | 3462.210 | 0.00000 | 8442 | 6074441 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4563 | 0.300 | 488.070 | 3462.210 | 0.00000 | 8442 | 6137508 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4583 | 0.680 | 194.910 | 3462.210 | 0.00000 | 8442 | 6200823 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4600 | 0.990 | 123.570 | 3462.210 | 0.00000 | 8442 | 6263945 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4607 | 0.300 | 430.250 | 3462.210 | 0.00000 | 8442 | 6326792 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4611 | 0.300 | 569.490 | 3462.210 | 0.00000 | 8442 | 6389865 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4644 | 0.300 | 595.070 | 3462.210 | 0.00000 | 8442 | 6453206 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4685 | 0.990 | 109.630 | 3462.210 | 0.00000 | 8442 | 6516685 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4708 | 0.490 | 115.620 | 3462.210 | 0.00000 | 8442 | 6576525 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4741 | 0.300 | 126.700 | 3462.210 | 0.00000 | 8442 | 6637483 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4771 | 0.990 | 222.790 | 3462.210 | 0.00000 | 8442 | 6699797 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4809 | 0.300 | 87.490 | 3462.210 | 0.00000 | 8442 | 6758240 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4815 | 7.790 | 124.810 | 3462.210 | 0.00000 | 8442 | 6797932 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4826 | 1.800 | 139.970 | 3462.210 | 0.00000 | 8442 | 6843166 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4840 | 0.300 | 189.990 | 3462.210 | 0.00000 | 8442 | 6887798 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4858 | 1.390 | 125.210 | 3462.210 | 0.00000 | 8442 | 6932386 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4866 | 4.270 | 153.910 | 3462.210 | 0.00000 | 8442 | 6976886 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4871 | 35.820 | 214.680 | 3462.210 | 0.00000 | 8442 | 7021949 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4885 | 0.300 | 288.810 | 3462.210 | 0.00000 | 8442 | 7067170 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4889 | 6.310 | 191.630 | 3462.210 | 0.00000 | 8442 | 7112668 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import flappy_bird_gym\n",
    "\n",
    "# 确保环境是 FlappyBird-v0\n",
    "env = gym.make(\"FlappyBird-v0\")\n",
    "import os\n",
    "import pygame\n",
    "\n",
    "# 将声音输出重定向到\"无声设备\"\n",
    "os.environ[\"SDL_AUDIODRIVER\"] = \"dummy\"  # 设置虚拟音频驱动\n",
    "pygame.mixer.quit()  # 重新初始化以应用设置\n",
    "\n",
    "# 定义支持点范围\n",
    "v_min = -1.0\n",
    "v_max = 11.5\n",
    "atoms = 51\n",
    "\n",
    "# 训练网络\n",
    "q_net, rewards = train_dueling_dqn_noise_MultiStep_PER(\n",
    "    env,\n",
    "    num_episodes=50000,\n",
    "    batch_size=512,\n",
    "    gamma=0.99,\n",
    "    epsilon_schedule=[(0, 1), (50000, 1), (150000, 0.01), (250000, 0.01), (350000, 0.001), (450000, 0.001), (500000, 0.0001), (650000, 0.0)],\n",
    "    lr=1e-4,\n",
    "    alpha=0.6,\n",
    "    beta_start=0.4,\n",
    "    beta_increment=1e-4\n",
    ")\n",
    "plot_dataList(rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import flappy_bird_gym\n",
    "import pygame\n",
    "\n",
    "if os.path.exists(fileName):  # 判断是否存在fileName文件\n",
    "    env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "    q_net = Dueling_NoisyDQN(input_dim, output_dim)\n",
    "    q_net.load_state_dict(torch.load(fileName, weights_only=True))\n",
    "    q_net.eval()\n",
    "    print(\"模型已加载\")\n",
    "    obs = env.reset()\n",
    "    bExit = False\n",
    "    current_score = 0\n",
    "    min_steps_between_flaps = 999999\n",
    "    steps_between_flaps = 0\n",
    "    while bExit == False:\n",
    "        obs = tuple(obs)\n",
    "        # Next action:\n",
    "        # (feed the observation to your agent here)\n",
    "        action = q_net(torch.tensor(obs, dtype=torch.float32).unsqueeze(0)).argmax().item()\n",
    "\n",
    "        # Processing:\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        steps_between_flaps += 1\n",
    "        if info['score'] > current_score:\n",
    "            current_score = info['score']\n",
    "            if steps_between_flaps < min_steps_between_flaps:\n",
    "                min_steps_between_flaps = steps_between_flaps\n",
    "            steps_between_flaps = 0\n",
    "        # Rendering the game:\n",
    "        # (remove this two lines during training)\n",
    "        env.render()\n",
    "        time.sleep(1 / 30)  # FPS\n",
    "\n",
    "        # 处理 pygame 事件队列，防止窗口卡死\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                env.close()\n",
    "                #exit()\n",
    "                bExit = True\n",
    "                done = True\n",
    "        \n",
    "        # Checking if the player is still alive\n",
    "        if done:\n",
    "            print(f\"score: {info['score']}\")\n",
    "            print(\"Game Over\")\n",
    "            steps_between_flaps = 0\n",
    "            obs = env.reset()\n",
    "\n",
    "    env.close()\n",
    "    print(f\"min_steps_between_flaps: {min_steps_between_flaps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
