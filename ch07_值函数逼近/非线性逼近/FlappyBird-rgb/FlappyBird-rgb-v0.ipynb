{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "# Q网络定义\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# 经验回放池\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (\n",
    "            torch.tensor(np.stack(states), dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.int64),\n",
    "            torch.tensor(rewards, dtype=torch.float32),\n",
    "            torch.tensor(np.stack(next_states), dtype=torch.float32),\n",
    "            torch.tensor(dones, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_dataList(rewards, xStart=0):\n",
    "    # 设置图像的宽度为 12 英寸\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(range(xStart, len(rewards) + xStart), rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Total Reward vs Episode')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.V = nn.Linear(128, 1)\n",
    "        self.A = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        Q = V + (A - A.mean(dim=1, keepdim=True))\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        \"\"\"\n",
    "        capacity: 缓冲区容量\n",
    "        alpha: 优先级比例，0表示完全随机采样，1表示完全按优先级采样\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"添加新经验，优先级初始化为最大值以确保被采样\"\"\"\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.priorities[self.position] = max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        \"\"\"\n",
    "        采样带优先级的批次数据\n",
    "        beta: 重要性采样的偏置修正参数\n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[:self.position]\n",
    "        \n",
    "        # 根据优先级分布计算采样概率\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        # 按照概率采样\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        # 计算重要性采样权重\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        # 解包样本\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        return (\n",
    "            torch.tensor(np.stack(states), dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.int64),\n",
    "            torch.tensor(rewards, dtype=torch.float32),\n",
    "            torch.tensor(np.stack(next_states), dtype=torch.float32),\n",
    "            torch.tensor(dones, dtype=torch.float32),\n",
    "            torch.tensor(weights, dtype=torch.float32),\n",
    "            indices,\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"根据新的 TD Error 更新优先级\"\"\"\n",
    "        self.priorities[indices] = priorities\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std_init=0.5):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "\n",
    "        # 可训练参数\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "\n",
    "        # 非参数化噪声\n",
    "        self.register_buffer(\"weight_epsilon\", torch.empty(out_features, in_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.empty(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # 初始化可训练参数\n",
    "        bound = 1 / self.in_features ** 0.5\n",
    "        self.weight_mu.data.uniform_(-bound, bound)\n",
    "        self.weight_sigma.data.fill_(self.std_init / (self.in_features ** 0.5))\n",
    "        self.bias_mu.data.uniform_(-bound, bound)\n",
    "        self.bias_sigma.data.fill_(self.std_init / (self.in_features ** 0.5))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        # 采样噪声\n",
    "        self.weight_epsilon.normal_()\n",
    "        self.bias_epsilon.normal_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return torch.nn.functional.linear(x, weight, bias)\n",
    "\n",
    "class Dueling_NoisyDQN(nn.Module):\n",
    "    def __init__(self, input_shape, output_dim):\n",
    "        super(Dueling_NoisyDQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),  # (C, H, W) -> Conv,输出：32@20x20\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # 输出：64@9x9\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), # 输出：64@7x7\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # 计算卷积后的特征图的尺寸\n",
    "        conv_output_size = self._get_conv_output_size(input_shape)\n",
    "\n",
    "        # Noisy 全连接层\n",
    "        self.fc = nn.Sequential(\n",
    "            NoisyLinear(conv_output_size, 512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Dueling 分支\n",
    "        self.V = NoisyLinear(512, 1)            # 状态价值分支\n",
    "        self.A = NoisyLinear(512, output_dim)   # 优势值分支\n",
    "\n",
    "    def _get_conv_output_size(self, shape):\n",
    "        x = torch.zeros(1, *shape)  # 临时张量用于计算\n",
    "        x = self.conv(x)\n",
    "        return int(torch.flatten(x, 1).size(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)  # 展平为向量\n",
    "        x = self.fc(x)\n",
    "\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        Q = V + (A - A.mean(dim=1, keepdim=True))\n",
    "        return Q\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, NoisyLinear):\n",
    "                m.reset_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepPrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, n_step=3, gamma=0.99):\n",
    "        \"\"\"\n",
    "        capacity: 缓冲区容量\n",
    "        alpha: 优先级比例，0表示完全随机采样，1表示完全按优先级采样\n",
    "        n_step: 多步时间跨度\n",
    "        gamma: 折扣因子\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.alpha = alpha\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # 用于多步存储的临时队列\n",
    "        self.n_step_queue = []\n",
    "\n",
    "    def _get_n_step_info(self):\n",
    "        \"\"\"从 n_step_queue 计算 n 步累计奖励和目标状态\"\"\"\n",
    "        R = 0\n",
    "        # 实际队列长度可能小于 n_step\n",
    "        n_step = len(self.n_step_queue)\n",
    "        for idx, (_, _, reward, _, _) in enumerate(self.n_step_queue):\n",
    "            R += (self.gamma ** idx) * reward\n",
    "        state, action, _, next_state, done = self.n_step_queue[0]\n",
    "        final_next_state, final_done = self.n_step_queue[-1][3], self.n_step_queue[-1][4]\n",
    "        return (state, action, R, final_next_state, final_done, n_step)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        添加新经验。\n",
    "        使用 n_step_queue 缓存多步数据，只有在积累到 n 步时才存入 buffer。\n",
    "        在轨迹结束时处理剩余的队列。\n",
    "        \"\"\"\n",
    "        self.n_step_queue.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # 如果 n_step_queue 满了，处理一个完整的 n-step 转移\n",
    "        if len(self.n_step_queue) == self.n_step:\n",
    "            n_step_transition = self._get_n_step_info()\n",
    "            max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append(n_step_transition)\n",
    "            else:\n",
    "                self.buffer[self.position] = n_step_transition\n",
    "            self.priorities[self.position] = max_priority\n",
    "            self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "            # 移除队列的第一个元素\n",
    "            self.n_step_queue.pop(0)\n",
    "\n",
    "        # 如果 done=True，处理剩余队列中的短步转移\n",
    "        if done:\n",
    "            while self.n_step_queue:\n",
    "                n_step_transition = self._get_n_step_info()\n",
    "                max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "                if len(self.buffer) < self.capacity:\n",
    "                    self.buffer.append(n_step_transition)\n",
    "                else:\n",
    "                    self.buffer[self.position] = n_step_transition\n",
    "                self.priorities[self.position] = max_priority\n",
    "                self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "                # 移除队列的第一个元素\n",
    "                self.n_step_queue.pop(0)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4, device='cpu'):\n",
    "        \"\"\"\n",
    "        采样带优先级的批次数据。\n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[:self.position]\n",
    "\n",
    "        # 根据优先级分布计算采样概率\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        # 按照概率采样\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        # 计算重要性采样权重\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        # 解包样本\n",
    "        states, actions, rewards, next_states, dones, n_steps = zip(*samples)\n",
    "        return (\n",
    "            torch.tensor(np.stack(states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(actions, dtype=torch.int64).to(device),\n",
    "            torch.tensor(rewards, dtype=torch.float32).to(device),\n",
    "            torch.tensor(np.stack(next_states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(dones, dtype=torch.float32).to(device),\n",
    "            torch.tensor(weights, dtype=torch.float32).to(device),\n",
    "            torch.tensor(n_steps, dtype=torch.int64).to(device),\n",
    "            indices,\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"根据新的 TD Error 更新优先级\"\"\"\n",
    "        self.priorities[indices] = priorities\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def current_queue_size(self):\n",
    "        return len(self.n_step_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess_image(frame, method='binarize'):\n",
    "    # 预处理图像：背景黑化(颜色值为200的像素点设为黑色)、灰度化、裁剪、缩放、二值化\n",
    "    frame[frame == 200] = 0\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame_cropped = frame_gray[:, :420]  # 裁剪掉地面部分\n",
    "    frame_resize = cv2.resize(frame_cropped, (84, 84))\n",
    "    if method == 'binarize':\n",
    "        processed_frame = cv2.threshold(frame_resize, 1, 255, cv2.THRESH_BINARY)[1]\n",
    "    else:\n",
    "        # 归一化到 [0, 1]\n",
    "        processed_frame = frame_resize / 255.0\n",
    "    return processed_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.3 (SDL 2.0.16, Python 3.8.20)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pygame\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import configparser\n",
    "\n",
    "fileName = 'models/fb_v0_no_score_2024-12-10_21-45-31.pth'\n",
    "bestScoreFileName = 'flappy_bird_v0_model_best_score.pth'\n",
    "stopTrainingFileName = 'flappy_bird_stop.txt'\n",
    "guideFileName = 'result1/models/fb_v0_no_score_2024-12-07_16-37-46.pth'\n",
    "\n",
    "def train_dueling_dqn_noise_MultiStep_PER(env, num_episodes=500, batch_size=64, gamma=0.99, \n",
    "                                          epsilon_schedule=[(0, 1.0), (20000, 0.1), (700000, 0.01), (1040000, 0.001), (1720000, 0.0001), (2060000, 0.0)], \n",
    "                                          lr=1e-3, alpha=0.6, beta_start=0.4, beta_increment=1e-4, \n",
    "                                          number_of_states=4, preprocessHeight=84, preprocessWidth=84, skip_frames=1):\n",
    "    # 新的状态维度为原始状态维度的 number_of_states 倍\n",
    "    input_shape = (number_of_states, preprocessHeight, preprocessWidth)\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    # 检查是否有GPU可用\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    q_net = Dueling_NoisyDQN(input_shape, output_dim).to(device)\n",
    "    # 判断是否存在fileName文件\n",
    "    if os.path.exists(fileName):\n",
    "        q_net.load_state_dict(torch.load(fileName, weights_only=True, map_location=device))\n",
    "        print(\"模型已加载\")\n",
    "    q_net.train() # 设置为训练模式，需要通过训练更新参数，该行代码可以省略，因为默认就是训练模式\n",
    "    target_net = Dueling_NoisyDQN(input_shape, output_dim).to(device)\n",
    "    target_net.load_state_dict(q_net.state_dict())\n",
    "    target_net.eval() # 设置为评估模式，不需要通过训练更新参数，更新时只需要复制q_net的参数\n",
    "\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
    "    gamma = gamma ** skip_frames  # 跳帧处理\n",
    "    replay_buffer_capacity = 100000\n",
    "    replay_buffer = MultiStepPrioritizedReplayBuffer(capacity=replay_buffer_capacity, alpha=alpha, n_step=20, gamma=gamma)\n",
    "    epsilon = epsilon_schedule[0][1]\n",
    "    beta = beta_start\n",
    "    rewards = []  # 确保它是一个列表\n",
    "    max_reward_total = -np.inf\n",
    "    max_interval_rewards = -np.inf\n",
    "    min_interval_rewards = np.inf\n",
    "    max_score = 0\n",
    "    max_step_count = 0\n",
    "    update_step_count = 0\n",
    "    update_step_interval = 200\n",
    "    print_interval = 300  # 间隔（单位：秒）\n",
    "    # 记录训练开始的时间\n",
    "    last_save_time = time.time()\n",
    "    last_print_time = last_save_time\n",
    "    stop_training = False\n",
    "    steps_Interval = 1000\n",
    "    steps_perInterval = 0\n",
    "    steps_total = 0\n",
    "    loss_perInterval = 0\n",
    "    q_value_perInterval = 0\n",
    "    delta_training_frequency = 3\n",
    "    delta_loss_threshold = 0.1\n",
    "    # 获取当前时间戳\n",
    "    current_time_str = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    csv_file = f'dueling_dqn_noise_MultiStep_PER_{current_time_str}.csv'\n",
    "    aim_score = 10\n",
    "\n",
    "    # 创建表格文件，列名分别为：总步数、epsilon、平均损失、平均Q值\n",
    "    with open(csv_file, 'w') as f:\n",
    "        f.write('Time,episode,Steps,epsilon,loss,Q_value\\n')\n",
    "        f.close()\n",
    "    ratio_schedule = []\n",
    "    for i in range(len(epsilon_schedule) - 1):\n",
    "        start_step, start_epsilon = epsilon_schedule[i]\n",
    "        end_step, end_epsilon = epsilon_schedule[i + 1]\n",
    "        ratio = (end_epsilon - start_epsilon) / (end_step - start_step)\n",
    "        ratio_schedule.append(ratio)\n",
    "    \n",
    "    print(f\"Episode | min interval reward | max interval reward | max_reward_total | Epsilon | max_score | steps_total | lr | update_step_interval | training_frequency | loss_threshold | beta | replay_buffer.size\")\n",
    "    for episode in range(num_episodes):\n",
    "        raw_state = env.reset()  # 返回 numpy.ndarray\n",
    "        processed_frame = preprocess_image(raw_state)\n",
    "        state_queue = deque([processed_frame.copy() for _ in range(number_of_states)], maxlen=number_of_states)  # 初始化队列，初始状态填充队列\n",
    "        state = np.array(state_queue)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        reward_perSkip = 0\n",
    "        score = 0\n",
    "\n",
    "        while not done:  # 每个 episode 的最大步数\n",
    "            # 根据多段线性衰减策略计算 epsilon\n",
    "            for i in range(len(epsilon_schedule) - 1):\n",
    "                start_step, start_epsilon = epsilon_schedule[i]\n",
    "                end_step, end_epsilon = epsilon_schedule[i + 1]\n",
    "                if start_step <= steps_total < end_step:\n",
    "                    # 在当前阶段内进行线性插值\n",
    "                    ratio = ratio_schedule[i]\n",
    "                    epsilon = max(start_epsilon + ratio * (steps_total - start_step), end_epsilon)\n",
    "            # 跳帧处理\n",
    "            if step_count % skip_frames == 0:\n",
    "                # ε-贪婪策略\n",
    "                if random.random() < epsilon:\n",
    "                    # 根据概率决定采样\n",
    "                    if random.random() < 0.08 * skip_frames:\n",
    "                        action = 1\n",
    "                    else:\n",
    "                        action = 0\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        action = q_net(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)).argmax().item()\n",
    "                action_perSkip = action\n",
    "            else:\n",
    "                action = 0\n",
    "\n",
    "            # 执行动作\n",
    "            next_raw_state, reward, done, info = env.step(action)\n",
    "            step_count += 1\n",
    "            steps_total += 1\n",
    "            bSkip = True\n",
    "            if step_count % skip_frames == 0:\n",
    "                bSkip = False\n",
    "                reward_perSkip = 0\n",
    "            if max_step_count < step_count:\n",
    "                max_step_count = step_count\n",
    "            raw_state = next_raw_state\n",
    "            if done:\n",
    "                bSkip = False\n",
    "            # 更新状态队列\n",
    "            processed_frame = preprocess_image(raw_state)\n",
    "            state_queue.append(processed_frame)\n",
    "            next_state = np.array(state_queue)\n",
    "            reward = reward * 0.1  # 缩放奖励\n",
    "            # 得分\n",
    "            bScore = False\n",
    "            if info['score'] > score:\n",
    "                reward += 0.4  # 奖励增加\n",
    "                score = info['score']\n",
    "                bScore = True\n",
    "                if score % aim_score == 0:\n",
    "                    bSkip = False\n",
    "            if info['score'] > max_score:\n",
    "                max_score = info['score']\n",
    "                if max_score > 100:\n",
    "                    torch.save(q_net.state_dict(), bestScoreFileName) # 保存模型\n",
    "            if done:\n",
    "                reward -= 0.2  # 惩罚\n",
    "            reward_perSkip += reward\n",
    "            if bSkip == False:\n",
    "                if bScore:\n",
    "                    if score % aim_score == 0:\n",
    "                        replay_buffer.add(state, action_perSkip, reward_perSkip, next_state, True) # 得到aim_score分结束，否则轨迹越长，Q值越大，无法收敛\n",
    "                    else:\n",
    "                        replay_buffer.add(state, action_perSkip, reward_perSkip, next_state, done)\n",
    "                else:\n",
    "                    replay_buffer.add(state, action_perSkip, reward_perSkip, next_state, done)\n",
    "                state = next_state\n",
    "            total_reward += reward\n",
    "            if total_reward > max_reward_total:\n",
    "                max_reward_total = total_reward\n",
    "            if max_interval_rewards < total_reward:\n",
    "                max_interval_rewards = total_reward\n",
    "\n",
    "            # 经验回放训练\n",
    "            if replay_buffer.size() >= batch_size and bSkip == False:\n",
    "                training_frequency = 1\n",
    "                loss_threshold = 0.5\n",
    "                while training_frequency > 0:\n",
    "                    # 从优先级缓冲区中采样\n",
    "                    states, actions, rewards_batch, next_states, dones, weights, indices, n_step_batch = replay_buffer.sample(batch_size, beta, device)\n",
    "\n",
    "                    q_values = q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                    with torch.no_grad():\n",
    "                        q_value_perInterval += q_values.mean().item() / steps_Interval\n",
    "                        best_actions = q_net(next_states).argmax(1)  # 使用当前网络选择最大Q值的动作\n",
    "                        target_q_values = rewards_batch + (gamma ** n_step_batch) * (1 - dones) * target_net(next_states).gather(1, best_actions.unsqueeze(1)).squeeze(1)\n",
    "                    # 计算 TD Error\n",
    "                    td_errors = target_q_values - q_values\n",
    "                    loss = (weights * td_errors.pow(2)).mean()\n",
    "                    loss_perInterval += loss.item() / steps_Interval\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # 更新优先级\n",
    "                    priorities = td_errors.abs().detach().cpu().numpy()\n",
    "                    replay_buffer.update_priorities(indices, priorities)\n",
    "                    update_step_count += 1\n",
    "                    steps_perInterval += 1\n",
    "                \n",
    "                    # 更新目标网络\n",
    "                    if update_step_count >= update_step_interval:\n",
    "                        update_step_count = 0\n",
    "                        target_net.load_state_dict(q_net.state_dict()) # 将q_net的参数复制到target_net中\n",
    "                    training_frequency -= 1\n",
    "                    if loss.item() > loss_threshold and replay_buffer.size() >= replay_buffer_capacity * 0.5 and training_frequency == 0: # 如果损失大于阈值，且经验池已足够大，则重复训练，暂停与环境互动\n",
    "                        training_frequency += delta_training_frequency\n",
    "                        loss_threshold += delta_loss_threshold\n",
    "                    if steps_perInterval >= steps_Interval:\n",
    "                        # 追加数据到 CSV 文件\n",
    "                        with open(csv_file, 'a') as f:\n",
    "                            current_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                            f.write(f'{current_time_str},{episode},{steps_total},{epsilon},{loss_perInterval},{q_value_perInterval}\\n')\n",
    "                        steps_perInterval = 0\n",
    "                        loss_perInterval = 0\n",
    "                        q_value_perInterval = 0\n",
    "                    # 检查时间间隔\n",
    "                    current_time = time.time()\n",
    "                    if current_time - last_save_time > 60:\n",
    "                        # 读取配置文件\n",
    "                        config = configparser.ConfigParser()\n",
    "                        if config.read('config.ini'):\n",
    "                            update_step_interval = config.getint('Training', 'update_step_interval')\n",
    "                            delta_training_frequency_temp = config.getint('Training', 'delta_training_frequency')\n",
    "                            if delta_training_frequency_temp >= 0:\n",
    "                                delta_training_frequency = delta_training_frequency_temp\n",
    "                            delta_loss_threshold_temp = config.getfloat('Training', 'delta_loss_threshold')\n",
    "                            if delta_loss_threshold_temp > 0:\n",
    "                                delta_loss_threshold = delta_loss_threshold_temp\n",
    "                            lr_temp = config.getfloat('Training', 'lr')\n",
    "                            if lr_temp > 0:\n",
    "                                lr = lr_temp\n",
    "                                state_dict = optimizer.state_dict()\n",
    "                                state_dict['param_groups'][0]['lr'] = lr\n",
    "                                optimizer.load_state_dict(state_dict)\n",
    "                            beta_temp = config.getfloat('Training', 'beta')\n",
    "                            if beta_temp > 0:\n",
    "                                beta = beta_temp\n",
    "                            stop_training = config.getboolean('Training', 'stop_training')\n",
    "                            guideOpen = config.getboolean('Training', 'guideOpen')\n",
    "                            \n",
    "                        # 设置保存路径和合法文件名\n",
    "                        save_path = \"./models\"\n",
    "                        os.makedirs(save_path, exist_ok=True)  # 确保路径存在\n",
    "                        current_time_str = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')  # 使用合法字符\n",
    "                        currentNetFile = os.path.join(save_path, f'fb_rgb_v0_{current_time_str}.pth')\n",
    "                        torch.save(q_net.state_dict(), currentNetFile) # 保存模型\n",
    "                        last_save_time = current_time\n",
    "                    if current_time - last_print_time >= print_interval:\n",
    "                        last_print_time = current_time\n",
    "                        print(f\"{episode} | {min_interval_rewards:.3f} | {max_interval_rewards:.3f} | {max_reward_total:.3f} | {epsilon:.5f} | {max_score} | {steps_total} | {lr:.8e} | {update_step_interval} | {training_frequency} | {loss_threshold:.3f} | {beta:.5f} | {replay_buffer.size()}\")\n",
    "                        min_interval_rewards = np.inf\n",
    "                        max_interval_rewards = -np.inf\n",
    "                    # 更新 beta\n",
    "                    beta = min(1.0, beta + beta_increment)\n",
    "\n",
    "        rewards.append(total_reward)  # 确保 append 正常工作\n",
    "        if min_interval_rewards > total_reward:\n",
    "            min_interval_rewards = total_reward\n",
    "        # 重置噪声\n",
    "        q_net.reset_noise()\n",
    "        target_net.reset_noise()\n",
    "        if stop_training:\n",
    "            # 把配置文件的stop_training改为False\n",
    "            config['Training']['stop_training'] = 'False'\n",
    "            with open('config.ini', 'w') as configfile:\n",
    "                config.write(configfile)\n",
    "            break\n",
    "\n",
    "    return q_net, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Dueling_Noisy_DistributionalDQN(nn.Module):\n",
    "    def __init__(self, input_shape, output_dim, num_atoms=51):\n",
    "        super(Dueling_Noisy_DistributionalDQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),  # (C, H, W) -> Conv,输出：32@20x20\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # 输出：64@9x9\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), # 输出：64@7x7\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # 计算卷积后的特征图的尺寸\n",
    "        conv_output_size = self._get_conv_output_size(input_shape)\n",
    "\n",
    "        # Noisy 全连接层\n",
    "        self.fc = nn.Sequential(\n",
    "            NoisyLinear(conv_output_size, 512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Dueling 分支\n",
    "        self.V = NoisyLinear(512, num_atoms)            # 状态价值分支\n",
    "        self.A = NoisyLinear(512, output_dim * num_atoms)   # 优势值分支\n",
    "        self.num_atoms = num_atoms\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def _get_conv_output_size(self, shape):\n",
    "        x = torch.zeros(1, *shape)  # 临时张量用于计算\n",
    "        x = self.conv(x)\n",
    "        return int(torch.flatten(x, 1).size(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)  # 展平为向量\n",
    "        x = self.fc(x)\n",
    "\n",
    "        V = self.V(x).view(-1, 1, self.num_atoms)\n",
    "        A = self.A(x).view(-1, self.output_dim, self.num_atoms)\n",
    "        Q = V + (A - A.mean(dim=1, keepdim=True))\n",
    "        Q_prob = F.softmax(Q, dim=2) # 将 Q 值转换为概率分布\n",
    "        return Q_prob\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, NoisyLinear):\n",
    "                m.reset_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_distribution(next_dist, rewards, dones, gammas, atoms, v_min, v_max, delta_z, support):\n",
    "    \"\"\"\n",
    "    投影 Bellman 更新后的分布到支持点。\n",
    "    \"\"\"\n",
    "    #delta_z = (v_max - v_min) / (atoms - 1)\n",
    "    #support = torch.linspace(v_min, v_max, atoms).to(next_dist.device)  # Shape: (atoms,)\n",
    "    \n",
    "    batch_size = rewards.size(0)\n",
    "    next_support = rewards.unsqueeze(1) + gammas.unsqueeze(1) * support.unsqueeze(0) * (1 - dones.unsqueeze(1))  # Shape: (batch_size, atoms)\n",
    "    next_support = next_support.clamp(v_min, v_max)  # 限制范围\n",
    "\n",
    "    b = (next_support - v_min) / delta_z  # Shape: (batch_size, atoms)\n",
    "    l = b.floor().long()  # Shape: (batch_size, atoms)\n",
    "    u = b.ceil().long()  # Shape: (batch_size, atoms)\n",
    "    \n",
    "    # 修正索引的范围，确保不越界\n",
    "    l = l.clamp(0, atoms - 1)\n",
    "    u = u.clamp(0, atoms - 1)\n",
    "    \n",
    "    proj_dist = torch.zeros(batch_size, atoms).to(next_dist.device)  # Shape: (batch_size, atoms)\n",
    "\n",
    "    for i in range(atoms):  # 遍历每个支持点\n",
    "        # 注意：next_dist[:, i] 实际是 batch_size 的第 i 列 (shape: [batch_size])\n",
    "        # next_dist 应被广播以匹配 l 和 u 的维度\n",
    "        weight_left = (u[:, i] - b[:, i]).unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "        weight_right = (b[:, i] - l[:, i]).unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "        \n",
    "        proj_dist.scatter_add_(1, l[:, i].unsqueeze(1), next_dist[:, i].unsqueeze(1) * weight_left)\n",
    "        proj_dist.scatter_add_(1, u[:, i].unsqueeze(1), next_dist[:, i].unsqueeze(1) * weight_right)\n",
    "\n",
    "    # 归一化分布\n",
    "    proj_dist /= proj_dist.sum(dim=1, keepdim=True) + 1e-8  # 防止除零\n",
    "    return proj_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rainbow_dqn(env, num_episodes=500, batch_size=64, gamma=0.99, \n",
    "                    epsilon_schedule=[(0, 1.0), (20000, 0.1), (700000, 0.01), (1040000, 0.001), (1720000, 0.0001), (2060000, 0.0)], \n",
    "                    lr=1e-3, alpha=0.6, beta_start=0.4, beta_increment=1e-4, \n",
    "                    number_of_states=4, preprocessHeight=84, preprocessWidth=84, skip_frames=1,\n",
    "                    atoms=51, v_min=-10, v_max=10,\n",
    "                    modelFile = None):\n",
    "    # 新的状态维度为原始状态维度的 number_of_states 倍\n",
    "    input_shape = (number_of_states, preprocessHeight, preprocessWidth)\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    # 检查是否有GPU可用\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    q_net = Dueling_Noisy_DistributionalDQN(input_shape, output_dim, atoms).to(device)\n",
    "    # 判断是否存在modelFile文件\n",
    "    if modelFile and os.path.exists(modelFile):\n",
    "        q_net.load_state_dict(torch.load(modelFile, weights_only=True, map_location=device))\n",
    "        print(\"模型已加载\")\n",
    "    q_net.train() # 设置为训练模式，需要通过训练更新参数，该行代码可以省略，因为默认就是训练模式\n",
    "    target_net = Dueling_Noisy_DistributionalDQN(input_shape, output_dim, atoms).to(device)\n",
    "    target_net.load_state_dict(q_net.state_dict())\n",
    "    target_net.eval() # 设置为评估模式，不需要通过训练更新参数，更新时只需要复制q_net的参数\n",
    "\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
    "    gamma = gamma ** skip_frames  # 跳帧处理\n",
    "    replay_buffer_capacity = 100000\n",
    "    replay_buffer = MultiStepPrioritizedReplayBuffer(capacity=replay_buffer_capacity, alpha=alpha, n_step=10, gamma=gamma)\n",
    "    delta_z = (v_max - v_min) / (atoms - 1)\n",
    "    supports = torch.linspace(v_min, v_max, atoms).to(device)\n",
    "    epsilon = epsilon_schedule[0][1]\n",
    "    beta = beta_start\n",
    "    rewards = []  # 确保它是一个列表\n",
    "    max_reward_total = -np.inf\n",
    "    max_interval_rewards = -np.inf\n",
    "    min_interval_rewards = np.inf\n",
    "    max_score = 0\n",
    "    max_step_count = 0\n",
    "    update_step_count = 0\n",
    "    update_step_interval = 200\n",
    "    print_interval = 300  # 间隔（单位：秒）\n",
    "    # 记录训练开始的时间\n",
    "    last_save_time = time.time()\n",
    "    last_print_time = last_save_time\n",
    "    stop_training = False\n",
    "    steps_Interval = 1000\n",
    "    steps_perInterval = 0\n",
    "    steps_total = 0\n",
    "    loss_perInterval = 0\n",
    "    q_value_perInterval = 0\n",
    "    delta_training_frequency = 3\n",
    "    delta_loss_threshold = 0.1\n",
    "    loss_threshold = 0.5\n",
    "    # 获取当前时间戳\n",
    "    current_time_str = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    csv_file = f'dueling_dqn_noise_MultiStep_PER_{current_time_str}.csv'\n",
    "\n",
    "    # 创建表格文件，列名分别为：总步数、epsilon、平均损失、平均Q值\n",
    "    with open(csv_file, 'w') as f:\n",
    "        f.write('Time,episode,Steps,epsilon,loss,Q_value\\n')\n",
    "        f.close()\n",
    "    ratio_schedule = []\n",
    "    for i in range(len(epsilon_schedule) - 1):\n",
    "        start_step, start_epsilon = epsilon_schedule[i]\n",
    "        end_step, end_epsilon = epsilon_schedule[i + 1]\n",
    "        ratio = (end_epsilon - start_epsilon) / (end_step - start_step)\n",
    "        ratio_schedule.append(ratio)\n",
    "    \n",
    "    print(f\"Episode | min interval reward | max interval reward | max_reward_total | Epsilon | max_score | steps_total | lr | update_step_interval | training_frequency | loss_threshold | beta | replay_buffer.size\")\n",
    "    for episode in range(num_episodes):\n",
    "        raw_state = env.reset()  # 返回 numpy.ndarray\n",
    "        processed_frame = preprocess_image(raw_state)\n",
    "        state_queue = deque([processed_frame.copy() for _ in range(number_of_states)], maxlen=number_of_states)  # 初始化队列，初始状态填充队列\n",
    "        state = np.array(state_queue)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        reward_perSkip = 0\n",
    "        score = 0\n",
    "\n",
    "        while not done:  # 每个 episode 的最大步数\n",
    "            # 根据多段线性衰减策略计算 epsilon\n",
    "            for i in range(len(epsilon_schedule) - 1):\n",
    "                start_step, start_epsilon = epsilon_schedule[i]\n",
    "                end_step, end_epsilon = epsilon_schedule[i + 1]\n",
    "                if start_step <= steps_total / skip_frames < end_step:\n",
    "                    # 在当前阶段内进行线性插值\n",
    "                    ratio = ratio_schedule[i]\n",
    "                    epsilon = max(start_epsilon + ratio * (steps_total / skip_frames - start_step), end_epsilon)\n",
    "            # 跳帧处理\n",
    "            if step_count % skip_frames == 0:\n",
    "                # ε-贪婪策略\n",
    "                if random.random() < epsilon:\n",
    "                    # 根据概率决定采样\n",
    "                    if random.random() < 0.07 * skip_frames:\n",
    "                        action = 1\n",
    "                    else:\n",
    "                        action = 0\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        dist = q_net(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device))\n",
    "                        action = (dist * supports).sum(dim=2).argmax().item()\n",
    "                action_perSkip = action\n",
    "            else:\n",
    "                action = 0\n",
    "\n",
    "            # 执行动作\n",
    "            next_raw_state, reward, done, info = env.step(action)\n",
    "            step_count += 1\n",
    "            steps_total += 1\n",
    "            bSkip = True\n",
    "            if step_count % skip_frames == 0:\n",
    "                bSkip = False\n",
    "                reward_perSkip = 0\n",
    "            if max_step_count < step_count:\n",
    "                max_step_count = step_count\n",
    "            raw_state = next_raw_state\n",
    "            if done:\n",
    "                bSkip = False\n",
    "            # 更新状态队列\n",
    "            processed_frame = preprocess_image(raw_state)\n",
    "            state_queue.append(processed_frame)\n",
    "            next_state = np.array(state_queue)\n",
    "            reward = reward * 0.01  # 缩放奖励\n",
    "            # 得分\n",
    "            bScore = False\n",
    "            if info['score'] > score:\n",
    "                reward += 0.04  # 奖励增加\n",
    "                score = info['score']\n",
    "                bScore = True\n",
    "            if info['score'] > max_score:\n",
    "                max_score = info['score']\n",
    "                if max_score > 100:\n",
    "                    torch.save(q_net.state_dict(), bestScoreFileName) # 保存模型\n",
    "            if done:\n",
    "                reward -= 0.02  # 惩罚\n",
    "            reward_perSkip += reward\n",
    "            if bSkip == False:\n",
    "                replay_buffer.add(state, action_perSkip, reward_perSkip, next_state, done)\n",
    "                state = next_state\n",
    "            total_reward += reward\n",
    "            if total_reward > max_reward_total:\n",
    "                max_reward_total = total_reward\n",
    "            if max_interval_rewards < total_reward:\n",
    "                max_interval_rewards = total_reward\n",
    "\n",
    "            # 经验回放训练\n",
    "            if replay_buffer.size() >= replay_buffer_capacity * 0.1 and bSkip == False:\n",
    "                training_frequency = 1\n",
    "                \n",
    "                while training_frequency > 0:\n",
    "                    # 从优先级缓冲区中采样\n",
    "                    states, actions, rewards_batch, next_states, dones, weights, n_step_batch, indices = replay_buffer.sample(batch_size, beta, device)\n",
    "\n",
    "                    # 计算 Q 网络的分布\n",
    "                    dist = q_net(states)\n",
    "                    q_dist = dist[range(batch_size), actions]\n",
    "                    with torch.no_grad():\n",
    "                        # 目标网络输出分布\n",
    "                        next_dist = target_net(next_states)  # Shape: (batch_size, num_actions, atoms)\n",
    "                        # 行为网络选择动作（Double-DQN）\n",
    "                        next_q_values = (q_net(next_states) * supports).sum(dim=2)  # Shape: (batch_size, num_actions)\n",
    "                        q_value_perInterval += next_q_values.mean().item() / steps_Interval\n",
    "                        next_actions = next_q_values.argmax(dim=1)  # Shape: (batch_size,)\n",
    "                        # 根据行为网络选择的动作提取目标分布\n",
    "                        next_dist = next_dist[range(batch_size), next_actions]  # Shape: (batch_size, atoms)\n",
    "\n",
    "                        # 投影分布\n",
    "                        target_dist = projection_distribution(next_dist, rewards_batch, dones, gamma ** n_step_batch, atoms, v_min, v_max, delta_z, supports)\n",
    "                    # KL 散度损失\n",
    "                    loss = -(target_dist * q_dist.log()).sum(dim=1) * weights\n",
    "                    loss = loss.mean()\n",
    "                    loss_perInterval += loss.item() / steps_Interval\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # 更新优先级\n",
    "                    # Wasserstein 距离计算\n",
    "                    td_errors = torch.sum((target_dist - q_dist) * supports, dim=1)  # [batch_size]\n",
    "                    priorities = td_errors.abs().detach().cpu().numpy()\n",
    "                    replay_buffer.update_priorities(indices, priorities)\n",
    "                    update_step_count += 1\n",
    "                    steps_perInterval += 1\n",
    "                \n",
    "                    # 更新目标网络\n",
    "                    if update_step_count >= update_step_interval:\n",
    "                        update_step_count = 0\n",
    "                        target_net.load_state_dict(q_net.state_dict()) # 将q_net的参数复制到target_net中\n",
    "                    training_frequency -= 1\n",
    "                    if loss.item() > loss_threshold and replay_buffer.size() >= replay_buffer_capacity * 0.5 and training_frequency == 0: # 如果损失大于阈值，且经验池已足够大，则重复训练，暂停与环境互动\n",
    "                        training_frequency += delta_training_frequency\n",
    "                        loss_threshold += delta_loss_threshold\n",
    "                    if steps_perInterval >= steps_Interval:\n",
    "                        # 追加数据到 CSV 文件\n",
    "                        with open(csv_file, 'a') as f:\n",
    "                            current_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                            f.write(f'{current_time_str},{episode},{steps_total},{epsilon},{loss_perInterval},{q_value_perInterval}\\n')\n",
    "                        steps_perInterval = 0\n",
    "                        loss_perInterval = 0\n",
    "                        q_value_perInterval = 0\n",
    "                    # 检查时间间隔\n",
    "                    current_time = time.time()\n",
    "                    if current_time - last_save_time > 60:\n",
    "                        # 读取配置文件\n",
    "                        config = configparser.ConfigParser()\n",
    "                        if config.read('config.ini'):\n",
    "                            update_step_interval = config.getint('Training', 'update_step_interval')\n",
    "                            delta_training_frequency_temp = config.getint('Training', 'delta_training_frequency')\n",
    "                            if delta_training_frequency_temp >= 0:\n",
    "                                delta_training_frequency = delta_training_frequency_temp\n",
    "                            delta_loss_threshold_temp = config.getfloat('Training', 'delta_loss_threshold')\n",
    "                            if delta_loss_threshold_temp > 0:\n",
    "                                delta_loss_threshold = delta_loss_threshold_temp\n",
    "                            loss_threshold = config.getfloat('Training', 'loss_threshold')\n",
    "                            lr_temp = config.getfloat('Training', 'lr')\n",
    "                            if lr_temp > 0:\n",
    "                                lr = lr_temp\n",
    "                                state_dict = optimizer.state_dict()\n",
    "                                state_dict['param_groups'][0]['lr'] = lr\n",
    "                                optimizer.load_state_dict(state_dict)\n",
    "                            beta_temp = config.getfloat('Training', 'beta')\n",
    "                            if beta_temp > 0:\n",
    "                                beta = beta_temp\n",
    "                            stop_training = config.getboolean('Training', 'stop_training')\n",
    "                            guideOpen = config.getboolean('Training', 'guideOpen')\n",
    "                            batch_size = config.getint('Training', 'batch_size')\n",
    "                            \n",
    "                        # 设置保存路径和合法文件名\n",
    "                        save_path = \"./models\"\n",
    "                        os.makedirs(save_path, exist_ok=True)  # 确保路径存在\n",
    "                        current_time_str = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')  # 使用合法字符\n",
    "                        currentNetFile = os.path.join(save_path, f'fb_rgb_v0_{current_time_str}.pth')\n",
    "                        torch.save(q_net.state_dict(), currentNetFile) # 保存模型\n",
    "                        last_save_time = current_time\n",
    "                    if current_time - last_print_time >= print_interval:\n",
    "                        last_print_time = current_time\n",
    "                        print(f\"{episode} | {min_interval_rewards:.3f} | {max_interval_rewards:.3f} | {max_reward_total:.3f} | {epsilon:.5f} | {max_score} | {steps_total} | {lr:.8e} | {update_step_interval} | {training_frequency} | {loss_threshold:.3f} | {beta:.5f} | {replay_buffer.size()}\")\n",
    "                        min_interval_rewards = np.inf\n",
    "                        max_interval_rewards = -np.inf\n",
    "                    # 更新 beta\n",
    "                    beta = min(1.0, beta + beta_increment)\n",
    "\n",
    "        rewards.append(total_reward)  # 确保 append 正常工作\n",
    "        if min_interval_rewards > total_reward:\n",
    "            min_interval_rewards = total_reward\n",
    "        # 重置噪声\n",
    "        q_net.reset_noise()\n",
    "        target_net.reset_noise()\n",
    "        if stop_training:\n",
    "            # 把配置文件的stop_training改为False\n",
    "            config['Training']['stop_training'] = 'False'\n",
    "            with open('config.ini', 'w') as configfile:\n",
    "                config.write(configfile)\n",
    "            break\n",
    "\n",
    "    return q_net, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "配置文件已创建并写入初始内容。\n",
      "Using device: cuda\n",
      "Episode | min interval reward | max interval reward | max_reward_total | Epsilon | max_score | steps_total | lr | update_step_interval | training_frequency | loss_threshold | beta | replay_buffer.size\n",
      "dones shape: torch.Size([256])\n",
      "rewards shape: torch.Size([256])\n",
      "next_support shape: torch.Size([256, 51])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mquit()  \u001b[38;5;66;03m# 重新初始化以应用设置\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 训练网络\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m q_net, rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rainbow_dqn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon_schedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m150000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m250000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m350000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m400000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta_increment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumber_of_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43matoms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m51\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodelFile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     53\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m plot_dataList(rewards)\n\u001b[0;32m     55\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[11], line 183\u001b[0m, in \u001b[0;36mtrain_rainbow_dqn\u001b[1;34m(env, num_episodes, batch_size, gamma, epsilon_schedule, lr, alpha, beta_start, beta_increment, number_of_states, preprocessHeight, preprocessWidth, skip_frames, atoms, v_min, v_max, modelFile)\u001b[0m\n\u001b[0;32m    181\u001b[0m td_errors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((target_dist \u001b[38;5;241m-\u001b[39m q_dist) \u001b[38;5;241m*\u001b[39m supports, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size]\u001b[39;00m\n\u001b[0;32m    182\u001b[0m priorities \u001b[38;5;241m=\u001b[39m td_errors\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m--> 183\u001b[0m \u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_priorities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriorities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m update_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    185\u001b[0m steps_perInterval \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[6], line 105\u001b[0m, in \u001b[0;36mMultiStepPrioritizedReplayBuffer.update_priorities\u001b[1;34m(self, indices, priorities)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_priorities\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, priorities):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"根据新的 TD Error 更新优先级\"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpriorities\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m priorities\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch_gym\\lib\\site-packages\\torch\\_tensor.py:1083\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import flappy_bird_gym\n",
    "import configparser\n",
    "\n",
    "# 创建ConfigParser对象\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# 添加配置项\n",
    "config['Training'] = {\n",
    "    'update_step_interval': '100',\n",
    "    'lr': '0.000001',\n",
    "    'delta_training_frequency': '0',\n",
    "    'delta_loss_threshold': '-0.1',\n",
    "    'loss_threshold' : '0.5',\n",
    "    'beta': '-0.1',\n",
    "    'guideopen': 'False',\n",
    "    'stop_training': 'False',\n",
    "    'batch_size': '256'\n",
    "}\n",
    "\n",
    "# 写入配置文件\n",
    "with open('config.ini', 'w') as configfile:\n",
    "    config.write(configfile)\n",
    "\n",
    "print(\"配置文件已创建并写入初始内容。\")\n",
    "\n",
    "# 确保环境是 FlappyBird-v0\n",
    "env = gym.make(\"FlappyBird-rgb-v0\")\n",
    "import os\n",
    "import pygame\n",
    "\n",
    "# 将声音输出重定向到\"无声设备\"\n",
    "os.environ[\"SDL_AUDIODRIVER\"] = \"dummy\"  # 设置虚拟音频驱动\n",
    "pygame.mixer.quit()  # 重新初始化以应用设置\n",
    "\n",
    "# 训练网络\n",
    "q_net, rewards = train_rainbow_dqn(\n",
    "    env,\n",
    "    num_episodes=50000,\n",
    "    batch_size=256,\n",
    "    gamma=0.99,\n",
    "    epsilon_schedule=[(0, 1), (20000, 1), (60000, 0.01), (100000, 0.01), (150000, 0.001), (250000, 0.001), (350000, 0.0001), (400000, 0.0)],\n",
    "    lr=1e-6,\n",
    "    alpha=0.6,\n",
    "    beta_start=0.4,\n",
    "    beta_increment=1e-4,\n",
    "    number_of_states=4,\n",
    "    skip_frames=5,\n",
    "    atoms=51,\n",
    "    v_min=-0.02,\n",
    "    v_max=2.8,\n",
    "    modelFile = None\n",
    ")\n",
    "plot_dataList(rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import flappy_bird_gym\n",
    "import pygame\n",
    "\n",
    "if os.path.exists(fileName):  # 判断是否存在fileName文件\n",
    "    env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "    input_shape = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "    q_net = Dueling_NoisyDQN(input_shape, output_dim)\n",
    "    q_net.load_state_dict(torch.load(fileName, weights_only=True))\n",
    "    q_net.eval()\n",
    "    print(\"模型已加载\")\n",
    "    obs = env.reset()\n",
    "    bExit = False\n",
    "    current_score = 0\n",
    "    min_steps_between_flaps = 999999\n",
    "    steps_between_flaps = 0\n",
    "    while bExit == False:\n",
    "        obs = tuple(obs)\n",
    "        # Next action:\n",
    "        # (feed the observation to your agent here)\n",
    "        action = q_net(torch.tensor(obs, dtype=torch.float32).unsqueeze(0)).argmax().item()\n",
    "\n",
    "        # Processing:\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        steps_between_flaps += 1\n",
    "        if info['score'] > current_score:\n",
    "            current_score = info['score']\n",
    "            if steps_between_flaps < min_steps_between_flaps:\n",
    "                min_steps_between_flaps = steps_between_flaps\n",
    "            steps_between_flaps = 0\n",
    "        # Rendering the game:\n",
    "        # (remove this two lines during training)\n",
    "        env.render()\n",
    "        time.sleep(1 / 30)  # FPS\n",
    "\n",
    "        # 处理 pygame 事件队列，防止窗口卡死\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                env.close()\n",
    "                #exit()\n",
    "                bExit = True\n",
    "                done = True\n",
    "        \n",
    "        # Checking if the player is still alive\n",
    "        if done:\n",
    "            print(f\"score: {info['score']}\")\n",
    "            print(\"Game Over\")\n",
    "            steps_between_flaps = 0\n",
    "            obs = env.reset()\n",
    "\n",
    "    env.close()\n",
    "    print(f\"min_steps_between_flaps: {min_steps_between_flaps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
