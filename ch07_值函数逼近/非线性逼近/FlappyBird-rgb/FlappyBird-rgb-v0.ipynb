{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "# Q网络定义\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# 经验回放池\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (\n",
    "            torch.tensor(np.stack(states), dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.int64),\n",
    "            torch.tensor(rewards, dtype=torch.float32),\n",
    "            torch.tensor(np.stack(next_states), dtype=torch.float32),\n",
    "            torch.tensor(dones, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_dataList(rewards, xStart=0):\n",
    "    # 设置图像的宽度为 12 英寸\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(range(xStart, len(rewards) + xStart), rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Total Reward vs Episode')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.V = nn.Linear(128, 1)\n",
    "        self.A = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        Q = V + (A - A.mean(dim=1, keepdim=True))\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        \"\"\"\n",
    "        capacity: 缓冲区容量\n",
    "        alpha: 优先级比例，0表示完全随机采样，1表示完全按优先级采样\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"添加新经验，优先级初始化为最大值以确保被采样\"\"\"\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.priorities[self.position] = max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        \"\"\"\n",
    "        采样带优先级的批次数据\n",
    "        beta: 重要性采样的偏置修正参数\n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[:self.position]\n",
    "        \n",
    "        # 根据优先级分布计算采样概率\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        # 按照概率采样\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        # 计算重要性采样权重\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        # 解包样本\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        return (\n",
    "            torch.tensor(np.stack(states), dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.int64),\n",
    "            torch.tensor(rewards, dtype=torch.float32),\n",
    "            torch.tensor(np.stack(next_states), dtype=torch.float32),\n",
    "            torch.tensor(dones, dtype=torch.float32),\n",
    "            torch.tensor(weights, dtype=torch.float32),\n",
    "            indices,\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"根据新的 TD Error 更新优先级\"\"\"\n",
    "        self.priorities[indices] = priorities\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std_init=0.5):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "\n",
    "        # 可训练参数\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "\n",
    "        # 非参数化噪声\n",
    "        self.register_buffer(\"weight_epsilon\", torch.empty(out_features, in_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.empty(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # 初始化可训练参数\n",
    "        bound = 1 / self.in_features ** 0.5\n",
    "        self.weight_mu.data.uniform_(-bound, bound)\n",
    "        self.weight_sigma.data.fill_(self.std_init / (self.in_features ** 0.5))\n",
    "        self.bias_mu.data.uniform_(-bound, bound)\n",
    "        self.bias_sigma.data.fill_(self.std_init / (self.in_features ** 0.5))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        # 采样噪声\n",
    "        self.weight_epsilon.normal_()\n",
    "        self.bias_epsilon.normal_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return torch.nn.functional.linear(x, weight, bias)\n",
    "\n",
    "class Dueling_NoisyDQN(nn.Module):\n",
    "    def __init__(self, input_shape, output_dim):\n",
    "        super(Dueling_NoisyDQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),  # (C, H, W) -> Conv,输出：32@20x20\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # 输出：64@9x9\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), # 输出：64@7x7\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # 计算卷积后的特征图的尺寸\n",
    "        conv_output_size = self._get_conv_output_size(input_shape)\n",
    "\n",
    "        # Noisy 全连接层\n",
    "        self.fc = nn.Sequential(\n",
    "            NoisyLinear(conv_output_size, 512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Dueling 分支\n",
    "        self.V = NoisyLinear(512, 1)            # 状态价值分支\n",
    "        self.A = NoisyLinear(512, output_dim)   # 优势值分支\n",
    "\n",
    "    def _get_conv_output_size(self, shape):\n",
    "        x = torch.zeros(1, *shape)  # 临时张量用于计算\n",
    "        x = self.conv(x)\n",
    "        return int(torch.flatten(x, 1).size(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)  # 展平为向量\n",
    "        x = self.fc(x)\n",
    "\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        Q = V + (A - A.mean(dim=1, keepdim=True))\n",
    "        return Q\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, NoisyLinear):\n",
    "                m.reset_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepPrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, n_step=3, gamma=0.99):\n",
    "        \"\"\"\n",
    "        capacity: 缓冲区容量\n",
    "        alpha: 优先级比例，0表示完全随机采样，1表示完全按优先级采样\n",
    "        n_step: 多步时间跨度\n",
    "        gamma: 折扣因子\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.alpha = alpha\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # 用于多步存储的临时队列\n",
    "        self.n_step_queue = []\n",
    "\n",
    "    def _get_n_step_info(self):\n",
    "        \"\"\"从 n_step_queue 计算 n 步累计奖励和目标状态\"\"\"\n",
    "        R = 0\n",
    "        for idx, (_, _, reward, _, _) in enumerate(self.n_step_queue):\n",
    "            R += (self.gamma ** idx) * reward\n",
    "        state, action, _, next_state, done = self.n_step_queue[0]\n",
    "        final_next_state, final_done = self.n_step_queue[-1][3], self.n_step_queue[-1][4]\n",
    "        return (state, action, R, final_next_state, final_done)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        添加新经验。\n",
    "        使用 n_step_queue 缓存多步数据，只有在积累到 n 步时才存入 buffer。\n",
    "        在轨迹结束时处理剩余的队列。\n",
    "        \"\"\"\n",
    "        self.n_step_queue.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # 如果 n_step_queue 满了，处理一个完整的 n-step 转移\n",
    "        if len(self.n_step_queue) == self.n_step:\n",
    "            n_step_transition = self._get_n_step_info()\n",
    "            max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append(n_step_transition)\n",
    "            else:\n",
    "                self.buffer[self.position] = n_step_transition\n",
    "            self.priorities[self.position] = max_priority\n",
    "            self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "            # 移除队列的第一个元素\n",
    "            self.n_step_queue.pop(0)\n",
    "\n",
    "        # 如果 done=True，处理剩余队列中的短步转移\n",
    "        if done:\n",
    "            while self.n_step_queue:\n",
    "                n_step_transition = self._get_n_step_info()\n",
    "                max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "                if len(self.buffer) < self.capacity:\n",
    "                    self.buffer.append(n_step_transition)\n",
    "                else:\n",
    "                    self.buffer[self.position] = n_step_transition\n",
    "                self.priorities[self.position] = max_priority\n",
    "                self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "                # 移除队列的第一个元素\n",
    "                self.n_step_queue.pop(0)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4, device='cpu'):\n",
    "        \"\"\"\n",
    "        采样带优先级的批次数据。\n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[:self.position]\n",
    "\n",
    "        # 根据优先级分布计算采样概率\n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        # 按照概率采样\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        # 计算重要性采样权重\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        # 解包样本\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        return (\n",
    "            torch.tensor(np.stack(states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(actions, dtype=torch.int64).to(device),\n",
    "            torch.tensor(rewards, dtype=torch.float32).to(device),\n",
    "            torch.tensor(np.stack(next_states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(dones, dtype=torch.float32).to(device),\n",
    "            torch.tensor(weights, dtype=torch.float32).to(device),\n",
    "            indices,\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"根据新的 TD Error 更新优先级\"\"\"\n",
    "        self.priorities[indices] = priorities\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def current_queue_size(self):\n",
    "        return len(self.n_step_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess_image(frame, method='binarize'):\n",
    "    # 预处理图像：灰度化、裁剪、缩放、二值化\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame_cropped = frame_gray[:, :420]  # 裁剪掉地面部分\n",
    "    frame_resize = cv2.resize(frame_cropped, (84, 84))\n",
    "    if method == 'binarize':\n",
    "        processed_frame = (frame_resize > 199).astype(np.uint8) #cv2.threshold(frame_resize, 199, 1, cv2.THRESH_BINARY)[1]\n",
    "    else:\n",
    "        # 归一化到 [0, 1]\n",
    "        processed_frame = frame_resize / 255.0\n",
    "    return processed_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.3 (SDL 2.0.16, Python 3.8.20)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pygame\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import configparser\n",
    "\n",
    "fileName = 'models/fb_v0_no_score_2024-12-10_21-45-31.pth'\n",
    "bestScoreFileName = 'flappy_bird_v0_model_best_score.pth'\n",
    "stopTrainingFileName = 'flappy_bird_stop.txt'\n",
    "guideFileName = 'result1/models/fb_v0_no_score_2024-12-07_16-37-46.pth'\n",
    "\n",
    "def train_dueling_dqn_noise_MultiStep_PER(env, num_episodes=500, batch_size=64, gamma=0.99, \n",
    "                                          epsilon_schedule=[(0, 1.0), (20000, 0.1), (700000, 0.01), (1040000, 0.001), (1720000, 0.0001), (2060000, 0.0)], \n",
    "                                          lr=1e-3, alpha=0.6, beta_start=0.4, beta_increment=1e-4, \n",
    "                                          number_of_states=4, preprocessHeight=84, preprocessWidth=84):\n",
    "    # 新的状态维度为原始状态维度的 number_of_states 倍\n",
    "    input_shape = (number_of_states, preprocessHeight, preprocessWidth)\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    # 检查是否有GPU可用\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    q_net = Dueling_NoisyDQN(input_shape, output_dim).to(device)\n",
    "    # 判断是否存在fileName文件\n",
    "    if os.path.exists(fileName):\n",
    "        q_net.load_state_dict(torch.load(fileName, weights_only=True, map_location=device))\n",
    "        print(\"模型已加载\")\n",
    "    q_net.train() # 设置为训练模式，需要通过训练更新参数，该行代码可以省略，因为默认就是训练模式\n",
    "    target_net = Dueling_NoisyDQN(input_shape, output_dim).to(device)\n",
    "    target_net.load_state_dict(q_net.state_dict())\n",
    "    target_net.eval() # 设置为评估模式，不需要通过训练更新参数，更新时只需要复制q_net的参数\n",
    "\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
    "    replay_buffer_capacity = 100000\n",
    "    replay_buffer = MultiStepPrioritizedReplayBuffer(capacity=replay_buffer_capacity, alpha=alpha, n_step=20, gamma=gamma)\n",
    "    epsilon = epsilon_schedule[0][1]\n",
    "    beta = beta_start\n",
    "    rewards = []  # 确保它是一个列表\n",
    "    max_reward_total = -np.inf\n",
    "    max_interval_rewards = -np.inf\n",
    "    min_interval_rewards = np.inf\n",
    "    max_score = 0\n",
    "    max_step_count = 0\n",
    "    update_step_count = 0\n",
    "    update_step_interval = 200\n",
    "    print_interval = 300  # 间隔（单位：秒）\n",
    "    # 记录训练开始的时间\n",
    "    last_save_time = time.time()\n",
    "    last_print_time = last_save_time\n",
    "    stop_training = False\n",
    "    steps_Interval = 1000\n",
    "    steps_perInterval = 0\n",
    "    steps_total = 0\n",
    "    loss_perInterval = 0\n",
    "    q_value_perInterval = 0\n",
    "    delta_training_frequency = 3\n",
    "    delta_loss_threshold = 0.1\n",
    "    # 获取当前时间戳\n",
    "    current_time_str = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    csv_file = f'dueling_dqn_noise_MultiStep_PER_{current_time_str}.csv'\n",
    "    aim_score = 5000\n",
    "\n",
    "    # 创建表格文件，列名分别为：总步数、epsilon、平均损失、平均Q值\n",
    "    with open(csv_file, 'w') as f:\n",
    "        f.write('Time,episode,Steps,epsilon,loss,Q_value\\n')\n",
    "        f.close()\n",
    "    ratio_schedule = []\n",
    "    for i in range(len(epsilon_schedule) - 1):\n",
    "        start_step, start_epsilon = epsilon_schedule[i]\n",
    "        end_step, end_epsilon = epsilon_schedule[i + 1]\n",
    "        ratio = (end_epsilon - start_epsilon) / (end_step - start_step)\n",
    "        ratio_schedule.append(ratio)\n",
    "    \n",
    "    print(f\"Episode | min interval reward | max interval reward | max_reward_total | Epsilon | max_score | steps_total | lr | update_step_interval | training_frequency | loss_threshold | beta | replay_buffer.size\")\n",
    "    for episode in range(num_episodes):\n",
    "        raw_state = env.reset()  # 返回 numpy.ndarray\n",
    "        processed_frame = preprocess_image(raw_state)\n",
    "        state_queue = deque([processed_frame.copy() for _ in range(number_of_states)], maxlen=number_of_states)  # 初始化队列，初始状态填充队列\n",
    "        state = np.array(state_queue)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        skip_frames = 1\n",
    "        reward_perSkip = 0\n",
    "        score = 0\n",
    "\n",
    "        while not done:  # 每个 episode 的最大步数\n",
    "            # 根据多段线性衰减策略计算 epsilon\n",
    "            for i in range(len(epsilon_schedule) - 1):\n",
    "                start_step, start_epsilon = epsilon_schedule[i]\n",
    "                end_step, end_epsilon = epsilon_schedule[i + 1]\n",
    "                if start_step <= steps_total < end_step:\n",
    "                    # 在当前阶段内进行线性插值\n",
    "                    ratio = ratio_schedule[i]\n",
    "                    epsilon = max(start_epsilon + ratio * (steps_total - start_step), end_epsilon)\n",
    "            # 跳帧处理\n",
    "            bSkip = True\n",
    "            if step_count % skip_frames == 0:\n",
    "                bSkip = False\n",
    "                reward_perSkip = 0\n",
    "                # ε-贪婪策略\n",
    "                if random.random() < epsilon:\n",
    "                    # 根据概率决定采样\n",
    "                    if random.random() < 0.08:\n",
    "                        action = 1\n",
    "                    else:\n",
    "                        action = 0\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        action = q_net(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)).argmax().item()\n",
    "            else:\n",
    "                action = 0\n",
    "\n",
    "            # 执行动作\n",
    "            next_raw_state, reward, done, info = env.step(action)\n",
    "            step_count += 1\n",
    "            steps_total += 1\n",
    "            if max_step_count < step_count:\n",
    "                max_step_count = step_count\n",
    "            raw_state = next_raw_state\n",
    "            if done:\n",
    "                bSkip = False\n",
    "            if bSkip == False:\n",
    "                # 更新状态队列\n",
    "                processed_frame = preprocess_image(raw_state)\n",
    "                state_queue.append(processed_frame)\n",
    "                next_state = np.array(state_queue)\n",
    "            reward = reward * 0.1  # 缩放奖励\n",
    "            # 得分\n",
    "            bScore = False\n",
    "            if info['score'] > score:\n",
    "                reward += 0.4  # 奖励增加\n",
    "                score = info['score']\n",
    "                bScore = True\n",
    "            if info['score'] > max_score:\n",
    "                max_score = info['score']\n",
    "                if max_score > 100:\n",
    "                    torch.save(q_net.state_dict(), bestScoreFileName) # 保存模型\n",
    "            if done:\n",
    "                reward -= 0.2  # 惩罚\n",
    "            reward_perSkip += reward\n",
    "            if bSkip == False:\n",
    "                if bScore:\n",
    "                    if score % aim_score == 0:\n",
    "                        replay_buffer.add(state, action, reward_perSkip, next_state, True) # 得到aim_score分结束，否则轨迹越长，Q值越大，无法收敛\n",
    "                    else:\n",
    "                        replay_buffer.add(state, action, reward_perSkip, next_state, done)\n",
    "                else:\n",
    "                    replay_buffer.add(state, action, reward_perSkip, next_state, done)\n",
    "                state = next_state\n",
    "            total_reward += reward\n",
    "            if total_reward > max_reward_total:\n",
    "                max_reward_total = total_reward\n",
    "            if max_interval_rewards < total_reward:\n",
    "                max_interval_rewards = total_reward\n",
    "\n",
    "            # 经验回放训练\n",
    "            if replay_buffer.size() >= batch_size and bSkip == False:\n",
    "                training_frequency = 1\n",
    "                loss_threshold = 0.5\n",
    "                while training_frequency > 0:\n",
    "                    # 从优先级缓冲区中采样\n",
    "                    states, actions, rewards_batch, next_states, dones, weights, indices = replay_buffer.sample(batch_size, beta, device)\n",
    "\n",
    "                    q_values = q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                    with torch.no_grad():\n",
    "                        q_value_perInterval += q_values.mean().item() / steps_Interval\n",
    "                        best_actions = q_net(next_states).argmax(1)  # 使用当前网络选择最大Q值的动作\n",
    "                        target_q_values = rewards_batch + gamma * (1 - dones) * target_net(next_states).gather(1, best_actions.unsqueeze(1)).squeeze(1)\n",
    "                    # 计算 TD Error\n",
    "                    td_errors = target_q_values - q_values\n",
    "                    loss = (weights * td_errors.pow(2)).mean()\n",
    "                    loss_perInterval += loss.item() / steps_Interval\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # 更新优先级\n",
    "                    priorities = td_errors.abs().detach().cpu().numpy()\n",
    "                    replay_buffer.update_priorities(indices, priorities)\n",
    "                    update_step_count += 1\n",
    "                    steps_perInterval += 1\n",
    "                \n",
    "                    # 更新目标网络\n",
    "                    if update_step_count >= update_step_interval:\n",
    "                        update_step_count = 0\n",
    "                        target_net.load_state_dict(q_net.state_dict()) # 将q_net的参数复制到target_net中\n",
    "                    training_frequency -= 1\n",
    "                    if loss.item() > loss_threshold and replay_buffer.size() >= replay_buffer_capacity * 0.5 and training_frequency == 0: # 如果损失大于阈值，且经验池已足够大，则重复训练，暂停与环境互动\n",
    "                        training_frequency += delta_training_frequency\n",
    "                        loss_threshold += delta_loss_threshold\n",
    "                    if steps_perInterval >= steps_Interval:\n",
    "                        # 追加数据到 CSV 文件\n",
    "                        with open(csv_file, 'a') as f:\n",
    "                            current_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                            f.write(f'{current_time_str},{episode},{steps_total},{epsilon},{loss_perInterval},{q_value_perInterval}\\n')\n",
    "                        steps_perInterval = 0\n",
    "                        loss_perInterval = 0\n",
    "                        q_value_perInterval = 0\n",
    "                    # 检查时间间隔\n",
    "                    current_time = time.time()\n",
    "                    if current_time - last_save_time > 60:\n",
    "                        # 读取配置文件\n",
    "                        config = configparser.ConfigParser()\n",
    "                        if config.read('config.ini'):\n",
    "                            update_step_interval = config.getint('Training', 'update_step_interval')\n",
    "                            delta_training_frequency_temp = config.getint('Training', 'delta_training_frequency')\n",
    "                            if delta_training_frequency_temp >= 0:\n",
    "                                delta_training_frequency = delta_training_frequency_temp\n",
    "                            delta_loss_threshold_temp = config.getfloat('Training', 'delta_loss_threshold')\n",
    "                            if delta_loss_threshold_temp > 0:\n",
    "                                delta_loss_threshold = delta_loss_threshold_temp\n",
    "                            lr_temp = config.getfloat('Training', 'lr')\n",
    "                            if lr_temp > 0:\n",
    "                                lr = lr_temp\n",
    "                                state_dict = optimizer.state_dict()\n",
    "                                state_dict['param_groups'][0]['lr'] = lr\n",
    "                                optimizer.load_state_dict(state_dict)\n",
    "                            beta_temp = config.getfloat('Training', 'beta')\n",
    "                            if beta_temp > 0:\n",
    "                                beta = beta_temp\n",
    "                            stop_training = config.getboolean('Training', 'stop_training')\n",
    "                            guideOpen = config.getboolean('Training', 'guideOpen')\n",
    "                            \n",
    "                        # 设置保存路径和合法文件名\n",
    "                        save_path = \"./models\"\n",
    "                        os.makedirs(save_path, exist_ok=True)  # 确保路径存在\n",
    "                        current_time_str = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')  # 使用合法字符\n",
    "                        currentNetFile = os.path.join(save_path, f'fb_v0_no_score_{current_time_str}.pth')\n",
    "                        torch.save(q_net.state_dict(), currentNetFile) # 保存模型\n",
    "                        last_save_time = current_time\n",
    "                    if current_time - last_print_time >= print_interval:\n",
    "                        last_print_time = current_time\n",
    "                        print(f\"{episode} | {min_interval_rewards:.3f} | {max_interval_rewards:.3f} | {max_reward_total:.3f} | {epsilon:.5f} | {max_score} | {steps_total} | {lr:.8e} | {update_step_interval} | {training_frequency} | {loss_threshold:.3f} | {beta:.5f} | {replay_buffer.size()}\")\n",
    "                        min_interval_rewards = np.inf\n",
    "                        max_interval_rewards = -np.inf\n",
    "                    # 更新 beta\n",
    "                    beta = min(1.0, beta + beta_increment)\n",
    "\n",
    "        rewards.append(total_reward)  # 确保 append 正常工作\n",
    "        if min_interval_rewards > total_reward:\n",
    "            min_interval_rewards = total_reward\n",
    "        # 重置噪声\n",
    "        q_net.reset_noise()\n",
    "        target_net.reset_noise()\n",
    "        if stop_training:\n",
    "            # 把配置文件的stop_training改为False\n",
    "            config['Training']['stop_training'] = 'False'\n",
    "            with open('config.ini', 'w') as configfile:\n",
    "                config.write(configfile)\n",
    "            break\n",
    "\n",
    "    return q_net, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Episode | min interval reward | max interval reward | max_reward_total | Epsilon | max_score | steps_total | lr | update_step_interval | training_frequency | loss_threshold | beta | replay_buffer.size\n",
      "308 | 3.000 | 14.000 | 14.000 | 0.45784 | 1 | 30121 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 15214\n",
      "572 | 9.900 | 14.000 | 14.000 | 0.08680 | 1 | 57337 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 28970\n",
      "816 | 9.900 | 14.600 | 14.600 | 0.04021 | 1 | 83218 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 42072\n",
      "1030 | 9.900 | 18.100 | 18.100 | 0.01000 | 2 | 106925 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 54042\n",
      "1222 | 9.900 | 18.100 | 18.100 | 0.01000 | 2 | 129440 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 65422\n",
      "1400 | 9.900 | 18.700 | 18.700 | 0.00973 | 2 | 151481 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 76554\n",
      "1569 | 9.900 | 18.100 | 18.700 | 0.00587 | 2 | 172969 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 87402\n",
      "1731 | 9.900 | 22.100 | 22.100 | 0.00208 | 3 | 193996 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 98037\n",
      "1888 | 9.900 | 22.100 | 22.100 | 0.00100 | 3 | 214948 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "2044 | 9.900 | 23.000 | 23.000 | 0.00100 | 3 | 236594 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "2197 | 9.900 | 36.300 | 36.300 | 0.00100 | 7 | 258695 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "2348 | 9.900 | 30.300 | 36.300 | 0.00100 | 7 | 280701 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "2499 | 9.900 | 27.200 | 36.300 | 0.00100 | 7 | 303074 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "2626 | 9.900 | 34.600 | 36.300 | 0.00100 | 7 | 323342 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "2760 | 9.900 | 30.300 | 36.300 | 0.00100 | 7 | 344091 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "2886 | 9.900 | 40.900 | 40.900 | 0.00100 | 8 | 365498 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3012 | 9.900 | 44.300 | 44.300 | 0.00100 | 9 | 387525 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3138 | 9.900 | 44.900 | 44.900 | 0.00100 | 9 | 410425 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3256 | 9.900 | 49.200 | 49.200 | 0.00100 | 10 | 433539 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3366 | 10.300 | 76.000 | 76.000 | 0.00088 | 16 | 456804 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3476 | 10.600 | 61.600 | 76.000 | 0.00046 | 16 | 479842 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "3570 | 10.400 | 84.200 | 84.200 | 0.00010 | 18 | 503410 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "3659 | 10.700 | 90.000 | 90.000 | 0.00008 | 20 | 526193 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "3751 | 9.900 | 77.700 | 90.000 | 0.00007 | 20 | 548735 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "3849 | 9.900 | 79.500 | 90.000 | 0.00005 | 20 | 572565 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "3931 | 9.900 | 95.900 | 95.900 | 0.00004 | 21 | 595133 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "4026 | 6.800 | 56.800 | 95.900 | 0.00002 | 21 | 617391 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "4107 | 9.500 | 109.000 | 109.000 | 0.00001 | 24 | 639426 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "4190 | 9.900 | 117.000 | 117.000 | 0.00000 | 26 | 661544 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "4265 | 9.900 | 94.300 | 117.000 | 0.00000 | 26 | 683579 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "4344 | 9.900 | 83.700 | 117.000 | 0.00000 | 26 | 705853 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "4428 | 9.900 | 94.400 | 117.000 | 0.00000 | 26 | 728136 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "4513 | 9.900 | 69.800 | 117.000 | 0.00000 | 26 | 750337 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "4592 | 10.700 | 135.000 | 135.000 | 0.00000 | 31 | 772575 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "4657 | 10.200 | 141.300 | 141.300 | 0.00000 | 32 | 794501 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "4733 | 9.900 | 127.200 | 141.300 | 0.00000 | 32 | 816494 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "4804 | 9.900 | 110.600 | 141.300 | 0.00000 | 32 | 838608 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "4870 | 10.500 | 124.700 | 141.300 | 0.00000 | 32 | 860763 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "4937 | 10.600 | 133.700 | 141.300 | 0.00000 | 32 | 883060 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5012 | 10.400 | 87.700 | 141.300 | 0.00000 | 32 | 905401 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5083 | 10.800 | 268.100 | 268.100 | 0.00000 | 63 | 927626 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5148 | 9.900 | 124.700 | 268.100 | 0.00000 | 63 | 950507 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5216 | 10.400 | 158.400 | 268.100 | 0.00000 | 63 | 973740 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5282 | 10.800 | 136.900 | 268.100 | 0.00000 | 63 | 996934 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5334 | 11.700 | 195.400 | 268.100 | 0.00000 | 63 | 1020182 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5398 | 10.700 | 227.100 | 268.100 | 0.00000 | 63 | 1043443 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5450 | 9.900 | 223.500 | 268.100 | 0.00000 | 63 | 1066373 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5502 | 10.200 | 239.500 | 268.100 | 0.00000 | 63 | 1089341 | 1.00000000e-05 | 100 | 0 | 0.500 | 1.00000 | 100000\n",
      "5563 | 10.600 | 153.900 | 268.100 | 0.00000 | 63 | 1112599 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5620 | 10.900 | 137.400 | 268.100 | 0.00000 | 63 | 1135702 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5680 | 10.000 | 218.900 | 268.100 | 0.00000 | 63 | 1159119 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5718 | 10.800 | 329.700 | 329.700 | 0.00000 | 78 | 1182042 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5762 | 11.600 | 258.400 | 329.700 | 0.00000 | 78 | 1205148 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5818 | 9.900 | 157.500 | 329.700 | 0.00000 | 78 | 1228668 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5873 | 9.900 | 147.600 | 329.700 | 0.00000 | 78 | 1251776 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5912 | 10.100 | 211.800 | 329.700 | 0.00000 | 78 | 1275797 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "5959 | 9.400 | 231.900 | 329.700 | 0.00000 | 78 | 1298950 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6012 | 9.900 | 174.900 | 329.700 | 0.00000 | 78 | 1323077 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6054 | 9.900 | 202.600 | 329.700 | 0.00000 | 78 | 1345224 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6107 | 9.900 | 168.200 | 329.700 | 0.00000 | 78 | 1366975 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6149 | 9.900 | 183.800 | 329.700 | 0.00000 | 78 | 1388891 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6203 | 10.700 | 161.500 | 329.700 | 0.00000 | 78 | 1410801 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6256 | 9.900 | 284.500 | 329.700 | 0.00000 | 78 | 1432859 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6295 | 10.500 | 247.700 | 329.700 | 0.00000 | 78 | 1454405 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6342 | 9.900 | 186.100 | 329.700 | 0.00000 | 78 | 1474999 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6382 | 10.100 | 233.800 | 329.700 | 0.00000 | 78 | 1496037 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6427 | 9.900 | 133.800 | 329.700 | 0.00000 | 78 | 1518204 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6470 | 9.900 | 231.300 | 329.700 | 0.00000 | 78 | 1540644 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6520 | 9.900 | 150.100 | 329.700 | 0.00000 | 78 | 1562976 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6560 | 9.900 | 296.900 | 329.700 | 0.00000 | 78 | 1584900 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6607 | 9.900 | 145.100 | 329.700 | 0.00000 | 78 | 1605323 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6657 | 9.900 | 272.400 | 329.700 | 0.00000 | 78 | 1626297 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6703 | 10.300 | 159.400 | 329.700 | 0.00000 | 78 | 1647242 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6753 | 10.200 | 180.400 | 329.700 | 0.00000 | 78 | 1668642 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n",
      "6798 | 13.900 | 182.100 | 329.700 | 0.00000 | 78 | 1690078 | 1.00000000e-05 | 100 | 0 | 0.600 | 1.00000 | 100000\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import flappy_bird_gym\n",
    "\n",
    "# 确保环境是 FlappyBird-v0\n",
    "env = gym.make(\"FlappyBird-rgb-v0\")\n",
    "import os\n",
    "import pygame\n",
    "\n",
    "# 将声音输出重定向到\"无声设备\"\n",
    "os.environ[\"SDL_AUDIODRIVER\"] = \"dummy\"  # 设置虚拟音频驱动\n",
    "pygame.mixer.quit()  # 重新初始化以应用设置\n",
    "\n",
    "# 训练网络\n",
    "q_net, rewards = train_dueling_dqn_noise_MultiStep_PER(\n",
    "    env,\n",
    "    num_episodes=50000,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    epsilon_schedule=[(0, 1), (50000, 0.1), (100000, 0.01), (150000, 0.01), (200000, 0.001), (450000, 0.001), (500000, 0.0001), (650000, 0.0)],\n",
    "    lr=1e-5,\n",
    "    alpha=0.6,\n",
    "    beta_start=0.4,\n",
    "    beta_increment=1e-4,\n",
    "    number_of_states=6\n",
    ")\n",
    "plot_dataList(rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import flappy_bird_gym\n",
    "import pygame\n",
    "\n",
    "if os.path.exists(fileName):  # 判断是否存在fileName文件\n",
    "    env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "    input_shape = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "    q_net = Dueling_NoisyDQN(input_shape, output_dim)\n",
    "    q_net.load_state_dict(torch.load(fileName, weights_only=True))\n",
    "    q_net.eval()\n",
    "    print(\"模型已加载\")\n",
    "    obs = env.reset()\n",
    "    bExit = False\n",
    "    current_score = 0\n",
    "    min_steps_between_flaps = 999999\n",
    "    steps_between_flaps = 0\n",
    "    while bExit == False:\n",
    "        obs = tuple(obs)\n",
    "        # Next action:\n",
    "        # (feed the observation to your agent here)\n",
    "        action = q_net(torch.tensor(obs, dtype=torch.float32).unsqueeze(0)).argmax().item()\n",
    "\n",
    "        # Processing:\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        steps_between_flaps += 1\n",
    "        if info['score'] > current_score:\n",
    "            current_score = info['score']\n",
    "            if steps_between_flaps < min_steps_between_flaps:\n",
    "                min_steps_between_flaps = steps_between_flaps\n",
    "            steps_between_flaps = 0\n",
    "        # Rendering the game:\n",
    "        # (remove this two lines during training)\n",
    "        env.render()\n",
    "        time.sleep(1 / 30)  # FPS\n",
    "\n",
    "        # 处理 pygame 事件队列，防止窗口卡死\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                env.close()\n",
    "                #exit()\n",
    "                bExit = True\n",
    "                done = True\n",
    "        \n",
    "        # Checking if the player is still alive\n",
    "        if done:\n",
    "            print(f\"score: {info['score']}\")\n",
    "            print(\"Game Over\")\n",
    "            steps_between_flaps = 0\n",
    "            obs = env.reset()\n",
    "\n",
    "    env.close()\n",
    "    print(f\"min_steps_between_flaps: {min_steps_between_flaps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
