{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Total Reward: 3, Steps: 8\n",
      "Episode 100, Total Reward: 2, Steps: 9\n",
      "Episode 150, Total Reward: 3, Steps: 8\n",
      "Episode 200, Total Reward: 3, Steps: 8\n",
      "Episode 250, Total Reward: 3, Steps: 8\n",
      "Episode 300, Total Reward: 3, Steps: 8\n",
      "Episode 350, Total Reward: 3, Steps: 8\n",
      "Episode 400, Total Reward: 3, Steps: 8\n",
      "Episode 450, Total Reward: 3, Steps: 8\n",
      "Episode 500, Total Reward: 3, Steps: 8\n",
      "Optimal Path: [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 网格世界环境\n",
    "class GridWorld:\n",
    "    def __init__(self, size=5, goal=(4, 4)):\n",
    "        self.size = size  # 网格大小\n",
    "        self.goal = goal  # 目标位置\n",
    "        self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # 上下左右\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = (0, 0)  # 起始位置\n",
    "        return self.agent_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        next_pos = (self.agent_pos[0] + self.actions[action][0],\n",
    "                    self.agent_pos[1] + self.actions[action][1])\n",
    "        # 边界限制\n",
    "        next_pos = (max(0, min(self.size - 1, next_pos[0])),\n",
    "                    max(0, min(self.size - 1, next_pos[1])))\n",
    "        self.agent_pos = next_pos\n",
    "        # 奖励\n",
    "        reward = 10 if self.agent_pos == self.goal else -1\n",
    "        done = self.agent_pos == self.goal\n",
    "        return self.agent_pos, reward, done\n",
    "\n",
    "# 线性逼近 Q 学习\n",
    "class LinearApproximationQLearning:\n",
    "    def __init__(self, action_dim, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.state_dim = len(self.get_features((0, 0)))  # 状态特征维度\n",
    "        self.action_dim = action_dim  # 动作数量\n",
    "        self.alpha = alpha  # 学习率\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.epsilon = epsilon  # 探索率\n",
    "        # 初始化权重\n",
    "        self.weights = np.random.randn(action_dim, self.state_dim) * 0.01\n",
    "\n",
    "    def get_features(self, state):\n",
    "        # 归一化位置 (x, y)，将状态映射为特征向量\n",
    "        x, y = state\n",
    "        return np.array([x / 4.0, y / 4.0, (x / 4.0) ** 2, (y / 4.0) ** 2, (x * y) / 16.0, 1.0])  # 多项式\n",
    "\n",
    "    def q_value(self, state, action):\n",
    "        # 计算 Q 值：线性函数 w^T * phi(s)\n",
    "        features = self.get_features(state)\n",
    "        return np.dot(self.weights[action], features)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)  # 探索\n",
    "        else:\n",
    "            # 贪婪选择最大 Q 值的动作\n",
    "            q_values = [self.q_value(state, a) for a in range(self.action_dim)]\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def update_incremental(self, state, action, reward, next_state, done):\n",
    "        # 提取特征向量\n",
    "        features = self.get_features(state)\n",
    "        # 计算目标\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            next_q_values = [self.q_value(next_state, a) for a in range(self.action_dim)]\n",
    "            target = reward + self.gamma * max(next_q_values)\n",
    "        # Q 学习更新权重\n",
    "        td_error = target - self.q_value(state, action)\n",
    "        self.weights[action] += self.alpha * td_error * features\n",
    "    \n",
    "    def update_batch(self, batch):\n",
    "        # batch: [(state, action, reward, next_state, done), ...]\n",
    "        total_gradients = np.zeros_like(self.weights)  # 梯度初始化\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            features = self.get_features(state)\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                next_q_values = [self.q_value(next_state, a) for a in range(self.action_dim)]\n",
    "                target = reward + self.gamma * max(next_q_values)\n",
    "            td_error = target - self.q_value(state, action)\n",
    "            total_gradients[action] += td_error * features\n",
    "        # 平均梯度更新权重\n",
    "        self.weights += self.alpha * total_gradients / len(batch)\n",
    "\n",
    "# 训练线性逼近 Q 学习\n",
    "def train(grid_world, agent, episodes=500):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        update_epsilon(agent, episode, episodes)\n",
    "        state = grid_world.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        # 步数统计\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = grid_world.step(action)\n",
    "            agent.update_incremental(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        rewards.append(total_reward)\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Steps: {steps}\")\n",
    "    return rewards\n",
    "\n",
    "def update_epsilon(agent, episode, max_episodes):\n",
    "    agent.epsilon = max(0.01, agent.epsilon * (1 - episode / max_episodes))\n",
    "\n",
    "# 初始化环境和代理\n",
    "grid_world = GridWorld(size=5, goal=(4, 4))\n",
    "agent = LinearApproximationQLearning(action_dim=4, epsilon=1.0)\n",
    "\n",
    "# 训练\n",
    "rewards = train(grid_world, agent, episodes=500)\n",
    "\n",
    "# 测试最优策略\n",
    "state = grid_world.reset()\n",
    "done = False\n",
    "path = [state]\n",
    "while not done:\n",
    "    action = agent.choose_action(state)\n",
    "    state, _, done = grid_world.step(action)\n",
    "    path.append(state)\n",
    "\n",
    "print(\"Optimal Path:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Total Reward: 3, Steps: 8\n",
      "Episode 100, Total Reward: 3, Steps: 8\n",
      "Episode 150, Total Reward: 3, Steps: 8\n",
      "Episode 200, Total Reward: 3, Steps: 8\n",
      "Episode 250, Total Reward: 3, Steps: 8\n",
      "Episode 300, Total Reward: -99, Steps: 110\n",
      "Episode 350, Total Reward: 3, Steps: 8\n",
      "Episode 400, Total Reward: 3, Steps: 8\n",
      "Episode 450, Total Reward: 3, Steps: 8\n",
      "Episode 500, Total Reward: 3, Steps: 8\n",
      "Optimal Path: [(0, 0), (1, 0), (2, 0), (2, 1), (3, 1), (3, 0), (3, 1), (3, 2), (3, 3), (4, 3), (4, 4)]\n"
     ]
    }
   ],
   "source": [
    "# 批量训练线性逼近 Q 学习\n",
    "def train_batch(grid_world, agent, episodes=500):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        update_epsilon(agent, episode, episodes)\n",
    "        state = grid_world.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        batch = []\n",
    "        # 步数统计\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = grid_world.step(action)\n",
    "            batch.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        rewards.append(total_reward)\n",
    "        # 批量更新\n",
    "        agent.update_batch(batch)\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Steps: {steps}\")\n",
    "    return rewards\n",
    "agent = LinearApproximationQLearning(action_dim=4, epsilon=1.0)\n",
    "\n",
    "# 训练\n",
    "rewards = train_batch(grid_world, agent, episodes=500)\n",
    "\n",
    "# 测试最优策略\n",
    "state = grid_world.reset()\n",
    "done = False\n",
    "path = [state]\n",
    "while not done:\n",
    "    action = agent.choose_action(state)\n",
    "    state, _, done = grid_world.step(action)\n",
    "    path.append(state)\n",
    "\n",
    "print(\"Optimal Path:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Total Reward: 3, Steps: 8\n",
      "Episode 100, Total Reward: 3, Steps: 8\n",
      "Episode 150, Total Reward: 3, Steps: 8\n",
      "Episode 200, Total Reward: 3, Steps: 8\n",
      "Episode 250, Total Reward: 3, Steps: 8\n",
      "Episode 300, Total Reward: 3, Steps: 8\n",
      "Episode 350, Total Reward: 3, Steps: 8\n",
      "Episode 400, Total Reward: 3, Steps: 8\n",
      "Episode 450, Total Reward: 3, Steps: 8\n",
      "Episode 500, Total Reward: 3, Steps: 8\n",
      "Optimal Path: [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def store(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# 经验回放+批量训练线性逼近 Q 学习\n",
    "def train_batch_ReplayBuffer(grid_world, agent, episodes=500, batch_size=8):\n",
    "    rewards = []\n",
    "    # 初始化经验池\n",
    "    buffer = ReplayBuffer(capacity=10000)\n",
    "    for episode in range(episodes):\n",
    "        update_epsilon(agent, episode, episodes)\n",
    "        state = grid_world.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        batch = []\n",
    "        # 步数统计\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = grid_world.step(action)\n",
    "            buffer.store((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            # 如果经验池足够大，开始采样并更新\n",
    "            if len(buffer) >= batch_size:\n",
    "                batch = buffer.sample(batch_size)\n",
    "                agent.update_batch(batch)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        rewards.append(total_reward)\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Steps: {steps}\")\n",
    "    return rewards\n",
    "\n",
    "# 训练\n",
    "rewards = train_batch_ReplayBuffer(grid_world, agent, episodes=500, batch_size=8)\n",
    "\n",
    "# 测试最优策略\n",
    "state = grid_world.reset()\n",
    "done = False\n",
    "path = [state]\n",
    "while not done:\n",
    "    action = agent.choose_action(state)\n",
    "    state, _, done = grid_world.step(action)\n",
    "    path.append(state)\n",
    "\n",
    "print(\"Optimal Path:\", path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
