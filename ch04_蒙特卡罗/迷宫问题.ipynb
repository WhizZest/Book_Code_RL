{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 行为策略和改进策略都是均匀随机策略，所以属于在线策略\n",
    "# 因为returns用列表保存了所有的累计回报，最后再计算平均值，所以不是增量式更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy: {(0, 0): 'right', (0, 1): 'right', (0, 2): 'down', (1, 2): 'down', (1, 0): 'down', (2, 0): 'right', (2, 1): 'right'}\n",
      "State Values: {(1, 2): -2.5563258232235704, (0, 2): -4.710059171597633, (0, 1): -5.653610771113831, (0, 0): -5.988, (1, 0): -5.780246913580247, (2, 2): 0.0, (2, 1): -3.4190140845070425, (2, 0): -5.005952380952381}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# 定义迷宫环境\n",
    "class MazeEnv:\n",
    "    def __init__(self):\n",
    "        # 初始化迷宫布局\n",
    "        self.grid = np.array([\n",
    "            ['S', '-', '-'],\n",
    "            ['-', '#', '-'],\n",
    "            ['-', '-', 'G']\n",
    "        ])\n",
    "        # 设置起始状态和目标状态\n",
    "        self.start_state = (0, 0)\n",
    "        self.goal_state = (2, 2)\n",
    "        # 定义可能的动作\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        # 判断当前状态是否为目标状态\n",
    "        return state == self.goal_state\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        # 根据当前状态和动作，计算下一个状态和奖励\n",
    "        x, y = state\n",
    "        if action == 'up': x = max(0, x - 1)\n",
    "        if action == 'down': x = min(2, x + 1)\n",
    "        if action == 'left': y = max(0, y - 1)\n",
    "        if action == 'right': y = min(2, y + 1)\n",
    "        \n",
    "        if self.grid[x, y] == '#':  # 遇到障碍物\n",
    "            return state, -1\n",
    "        return (x, y), -1\n",
    "\n",
    "# 蒙特卡罗控制\n",
    "def monte_carlo_control(env, episodes=1000, gamma=1.0):\n",
    "    \"\"\"\n",
    "    使用蒙特卡罗控制方法学习迷宫的最优策略。\n",
    "\n",
    "    参数:\n",
    "    env (MazeEnv): 迷宫环境对象。\n",
    "    episodes (int): 训练的episode数量，默认为1000。\n",
    "    gamma (float): 折扣因子，默认为1.0。\n",
    "\n",
    "    返回:\n",
    "    policy (defaultdict): 学习到的最优策略。\n",
    "    V (defaultdict): 每个状态的值函数。\n",
    "    \"\"\"\n",
    "    # 初始化值函数和回报列表\n",
    "    V = defaultdict(float)\n",
    "    returns = defaultdict(list)\n",
    "    # 初始化策略，每个状态随机选择一个动作\n",
    "    policy = defaultdict(lambda: np.random.choice(env.actions))\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        # 初始化当前状态为起始状态\n",
    "        state = env.start_state\n",
    "        # 初始化轨迹\n",
    "        episode = []\n",
    "        \n",
    "        # 生成轨迹\n",
    "        while not env.is_terminal(state):\n",
    "            # 在当前状态下随机选择一个动作\n",
    "            while True:\n",
    "                action = np.random.choice(env.actions)\n",
    "                # 执行动作，获取下一个状态和奖励\n",
    "                next_state, reward = env.step(state, action)\n",
    "                # 如果下一个状态不等于当前状态，则更新策略并退出循环\n",
    "                if next_state != state:\n",
    "                    policy[state] = action\n",
    "                    break\n",
    "            # 将当前状态、动作和奖励添加到轨迹中\n",
    "            episode.append((state, action, reward))\n",
    "            # 更新当前状态为下一个状态\n",
    "            state = next_state\n",
    "        \n",
    "        # 计算每个状态的累计回报\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for state, _, reward in reversed(episode):\n",
    "            G = reward + gamma * G\n",
    "            if state not in visited:\n",
    "                visited.add(state)\n",
    "                returns[state].append(G)\n",
    "                V[state] = np.mean(returns[state])\n",
    "        \n",
    "        # 策略改进\n",
    "        for state, _, _ in episode:\n",
    "            policy[state] = max(env.actions, key=lambda a: expected_return(env, state, a, V, gamma))\n",
    "    \n",
    "    return policy, V\n",
    "\n",
    "def expected_return(env, state, action, V, gamma):\n",
    "    \"\"\"\n",
    "    计算给定状态和动作的期望回报。\n",
    "\n",
    "    参数:\n",
    "    env (MazeEnv): 迷宫环境对象。\n",
    "    state (tuple): 当前状态。\n",
    "    action (str): 当前动作。\n",
    "    V (defaultdict): 每个状态的值函数。\n",
    "    gamma (float): 折扣因子。\n",
    "\n",
    "    返回:\n",
    "    float: 期望回报。\n",
    "    \"\"\"\n",
    "    # 计算期望回报\n",
    "    next_state, reward = env.step(state, action)\n",
    "    return reward + gamma * V[next_state]\n",
    "\n",
    "# 运行\n",
    "env = MazeEnv()\n",
    "policy, value_function = monte_carlo_control(env)\n",
    "print(\"Learned Policy:\", dict(policy))\n",
    "print(\"State Values:\", dict(value_function))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 行为策略是均匀随机策略，改进策略为贪心策略，所以属于离线策略\n",
    "# 值函数采用增量式更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy:\n",
      "State (2, 1): right\n",
      "State (2, 0): right\n",
      "State (1, 2): down\n",
      "State (0, 2): down\n",
      "State (0, 1): right\n",
      "State (1, 0): down\n",
      "State (0, 0): down\n",
      "\n",
      "Action-Value Function:\n",
      "State ((2, 1), 'right'): -1.00\n",
      "State ((2, 0), 'right'): -2.00\n",
      "State ((2, 0), 'left'): -3.00\n",
      "State ((1, 2), 'down'): -1.00\n",
      "State ((1, 2), 'left'): -2.00\n",
      "State ((2, 1), 'down'): -2.00\n",
      "State ((1, 2), 'right'): -2.00\n",
      "State ((2, 0), 'down'): -3.00\n",
      "State ((0, 2), 'down'): -2.00\n",
      "State ((0, 1), 'right'): -3.00\n",
      "State ((0, 1), 'down'): -4.00\n",
      "State ((0, 2), 'left'): -4.00\n",
      "State ((0, 2), 'right'): -3.00\n",
      "State ((1, 0), 'down'): -3.00\n",
      "State ((0, 0), 'down'): -4.00\n",
      "State ((0, 1), 'left'): -5.00\n",
      "State ((2, 1), 'up'): -2.00\n",
      "State ((1, 2), 'up'): -3.00\n",
      "State ((0, 0), 'right'): -4.00\n",
      "State ((0, 2), 'up'): -3.00\n",
      "State ((2, 0), 'up'): -4.00\n",
      "State ((1, 0), 'left'): -4.00\n",
      "State ((0, 0), 'left'): -5.00\n",
      "State ((2, 1), 'left'): -3.00\n",
      "State ((0, 1), 'up'): -4.00\n",
      "State ((1, 0), 'right'): -4.00\n",
      "State ((0, 0), 'up'): -5.00\n",
      "State ((1, 0), 'up'): -5.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# 行为策略（随机策略）和目标策略（贪婪策略）\n",
    "def behavior_policy(state):\n",
    "    return np.random.choice(env.actions)\n",
    "\n",
    "def target_policy(state, V, gamma=1.0):\n",
    "    best_action = max(env.actions, key=lambda a: expected_return(env, state, a, V, gamma))\n",
    "    return best_action\n",
    "\n",
    "# 计算期望回报（假设确定性）\n",
    "def expected_return(env, state, action, V, gamma):\n",
    "    next_state, reward = env.step(state, action)\n",
    "    return reward + gamma * V[next_state]\n",
    "\n",
    "# 离线策略蒙特卡罗评估（加权重要性采样）\n",
    "def off_policy_monte_carlo_control(env, episodes=1000, gamma=1.0):\n",
    "    Q = defaultdict(float)\n",
    "    C = defaultdict(float)\n",
    "    policy = defaultdict(lambda: np.random.choice(env.actions))  # 初始化目标策略为随机\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        # 生成轨迹\n",
    "        state = env.start_state\n",
    "        episode = []\n",
    "        \n",
    "        while not env.is_terminal(state):\n",
    "            action = behavior_policy(state)\n",
    "            next_state, reward = env.step(state, action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        \n",
    "        # 计算每个状态的加权重要性回报\n",
    "        G = 0\n",
    "        W = 1  # 累积重要性权重\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = reward + gamma * G  # 累计回报\n",
    "            \n",
    "            # 更新累积加权重要性\n",
    "            C[(state, action)] += W\n",
    "            Q[(state, action)] += (W / C[(state, action)]) * (G - Q[(state, action)])\n",
    "            \n",
    "            # 更新策略为贪婪策略\n",
    "            policy[state] = max(env.actions, key=lambda a: Q.get((state, a), float('-inf')))\n",
    "\n",
    "            \n",
    "            # 计算重要性采样权重\n",
    "            if action != policy[state]:\n",
    "                break  # 如果动作偏离目标策略，停止累积权重\n",
    "            W *= 1.0 / 0.25  # 假设行为策略是随机的，行为策略的概率1/4\n",
    "    \n",
    "    return policy, Q\n",
    "\n",
    "# 运行环境和离线策略控制\n",
    "env = MazeEnv()\n",
    "policy, Q = off_policy_monte_carlo_control(env)\n",
    "\n",
    "print(\"Learned Policy:\")\n",
    "for state in policy:\n",
    "    print(f\"State {state}: {policy[state]}\")\n",
    "\n",
    "print(\"\\nAction-Value Function:\")\n",
    "for state_action, value in Q.items():\n",
    "    print(f\"State {state_action}: {value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 行为策略是ε-贪心策略，策略评估用的是求平均值，所以目标策略和行为策略一样，所以属于在线策略\n",
    "# 策略评估用的是求平均值，但并没有存储所有的累积回报，而是做了累加，所以应该也属于增量式更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy:\n",
      "State (0, 0): down\n",
      "State (0, 1): right\n",
      "State (0, 2): down\n",
      "State (1, 0): down\n",
      "State (1, 2): down\n",
      "State (2, 0): right\n",
      "State (2, 1): right\n",
      "\n",
      "Action-Value Function:\n",
      "State ((0, 2), 'up'): -10.33\n",
      "State ((0, 2), 'down'): -4.51\n",
      "State ((0, 2), 'left'): -13.44\n",
      "State ((0, 2), 'right'): -9.74\n",
      "State ((1, 2), 'down'): -1.00\n",
      "State ((0, 1), 'right'): -8.99\n",
      "State ((0, 1), 'down'): -13.83\n",
      "State ((0, 1), 'up'): -14.26\n",
      "State ((0, 0), 'right'): -13.22\n",
      "State ((0, 1), 'left'): -15.37\n",
      "State ((1, 0), 'up'): -13.91\n",
      "State ((0, 0), 'down'): -12.01\n",
      "State ((0, 0), 'up'): -15.37\n",
      "State ((0, 0), 'left'): -15.32\n",
      "State ((1, 0), 'down'): -8.06\n",
      "State ((1, 0), 'left'): -12.99\n",
      "State ((1, 0), 'right'): -13.05\n",
      "State ((1, 2), 'up'): -10.57\n",
      "State ((1, 2), 'left'): -6.16\n",
      "State ((1, 2), 'right'): -6.12\n",
      "State ((2, 1), 'up'): -5.76\n",
      "State ((2, 1), 'down'): -5.99\n",
      "State ((2, 1), 'left'): -9.80\n",
      "State ((2, 1), 'right'): -1.00\n",
      "State ((2, 0), 'right'): -4.09\n",
      "State ((2, 0), 'up'): -12.22\n",
      "State ((2, 0), 'down'): -9.57\n",
      "State ((2, 0), 'left'): -9.49\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# ε-贪婪策略\n",
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(env.actions)  # 随机动作\n",
    "    else:\n",
    "        return max(env.actions, key=lambda a: Q[(state, a)])  # 贪婪选择\n",
    "\n",
    "# 在线策略蒙特卡罗控制\n",
    "def on_policy_monte_carlo_control(env, episodes=1000, gamma=1.0, epsilon=0.1):\n",
    "    Q = defaultdict(float)  # 状态-动作值函数\n",
    "    returns_sum = defaultdict(float)  # 累计回报总和\n",
    "    returns_count = defaultdict(int)  # 回报计数\n",
    "    \n",
    "    policy = defaultdict(lambda: np.random.choice(env.actions))  # 初始化随机策略\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        # 生成轨迹\n",
    "        state = env.start_state\n",
    "        episode = []\n",
    "        \n",
    "        while not env.is_terminal(state):\n",
    "            action = epsilon_greedy_policy(state, Q, epsilon)\n",
    "            next_state, reward = env.step(state, action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        \n",
    "        # 计算回合中的每个状态-动作对的回报\n",
    "        G = 0\n",
    "        visited = set()  # 记录首次访问\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = reward + gamma * G  # 累计回报\n",
    "            \n",
    "            if (state, action) not in visited:\n",
    "                returns_sum[(state, action)] += G\n",
    "                returns_count[(state, action)] += 1\n",
    "                Q[(state, action)] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
    "                visited.add((state, action))\n",
    "        \n",
    "        # 更新策略（ε-贪婪）\n",
    "        for state, action, _ in episode:\n",
    "            policy[state] = max(env.actions, key=lambda a: Q[(state, a)])\n",
    "    \n",
    "    return policy, Q\n",
    "\n",
    "# 运行环境和在线策略控制\n",
    "env = MazeEnv()\n",
    "policy, Q = on_policy_monte_carlo_control(env, episodes=5000, epsilon=0.9)\n",
    "\n",
    "print(\"Learned Policy:\")\n",
    "for state in sorted(policy):\n",
    "    print(f\"State {state}: {policy[state]}\")\n",
    "\n",
    "print(\"\\nAction-Value Function:\")\n",
    "for state_action, value in Q.items():\n",
    "    print(f\"State {state_action}: {value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 行为策略和目标策略一样，都是ε-贪婪，属于在线策略，但策略改进只保留概率最大的行为\n",
    "# 策略评估属于增量式更新\n",
    "# 动态调整ε：随着训练的进行，逐渐线性减小ε的值，以减少探索，增加利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy:\n",
      "State (0, 0): down\n",
      "State (0, 1): right\n",
      "State (0, 2): down\n",
      "State (1, 0): down\n",
      "State (1, 2): down\n",
      "State (2, 0): right\n",
      "State (2, 1): right\n",
      "\n",
      "Action-Value Function:\n",
      "State ((2, 1), 'right'): -1.00\n",
      "State ((2, 1), 'up'): -4.88\n",
      "State ((2, 0), 'right'): -2.54\n",
      "State ((2, 0), 'down'): -7.53\n",
      "State ((2, 1), 'left'): -6.84\n",
      "State ((1, 0), 'down'): -4.51\n",
      "State ((2, 0), 'up'): -8.80\n",
      "State ((2, 0), 'left'): -6.59\n",
      "State ((1, 0), 'left'): -10.33\n",
      "State ((0, 0), 'down'): -6.70\n",
      "State ((0, 0), 'left'): -11.35\n",
      "State ((0, 0), 'up'): -11.94\n",
      "State ((0, 1), 'left'): -13.06\n",
      "State ((0, 0), 'right'): -10.73\n",
      "State ((1, 0), 'up'): -10.55\n",
      "State ((1, 0), 'right'): -10.15\n",
      "State ((0, 1), 'up'): -13.51\n",
      "State ((0, 1), 'down'): -13.10\n",
      "State ((0, 2), 'left'): -12.47\n",
      "State ((0, 2), 'up'): -9.64\n",
      "State ((0, 2), 'right'): -10.86\n",
      "State ((0, 1), 'right'): -7.44\n",
      "State ((0, 2), 'down'): -3.62\n",
      "State ((2, 1), 'down'): -4.61\n",
      "State ((1, 2), 'down'): -1.00\n",
      "State ((1, 2), 'left'): -3.32\n",
      "State ((1, 2), 'up'): -8.59\n",
      "State ((1, 2), 'right'): -5.53\n"
     ]
    }
   ],
   "source": [
    "# 在线策略蒙特卡罗控制，动态调整 ε\n",
    "def on_policy_monte_carlo_control(env, episodes=1000, gamma=1.0, epsilon_start=1.0, epsilon_min=0.1, decay_rate=0.001):\n",
    "    Q = defaultdict(float)\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(int)\n",
    "    \n",
    "    policy = defaultdict(lambda: np.random.choice(env.actions))\n",
    "    \n",
    "    for episode_num in range(episodes):\n",
    "        # 动态调整 ε\n",
    "        epsilon = max(epsilon_min, epsilon_start - decay_rate * episode_num)\n",
    "        \n",
    "        # 生成轨迹\n",
    "        state = env.start_state\n",
    "        episode = []\n",
    "        \n",
    "        while not env.is_terminal(state):\n",
    "            action = epsilon_greedy_policy(state, Q, epsilon)\n",
    "            next_state, reward = env.step(state, action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        \n",
    "        # 计算每个状态-动作对的回报\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = reward + gamma * G\n",
    "            if (state, action) not in visited:\n",
    "                returns_sum[(state, action)] += G\n",
    "                returns_count[(state, action)] += 1\n",
    "                Q[(state, action)] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
    "                visited.add((state, action))\n",
    "        \n",
    "        # 更新策略\n",
    "        for state, _, _ in episode:\n",
    "            policy[state] = max(env.actions, key=lambda a: Q[(state, a)])\n",
    "    \n",
    "    return policy, Q\n",
    "\n",
    "# 运行环境和在线策略控制\n",
    "env = MazeEnv()\n",
    "policy, Q = on_policy_monte_carlo_control(env, episodes=1000, epsilon_start=1.0, epsilon_min=0.1, decay_rate=0.001)\n",
    "\n",
    "print(\"Learned Policy:\")\n",
    "for state in sorted(policy):\n",
    "    print(f\"State {state}: {policy[state]}\")\n",
    "\n",
    "print(\"\\nAction-Value Function:\")\n",
    "for state_action, value in Q.items():\n",
    "    print(f\"State {state_action}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 行为策略和目标策略一样，都是ε-贪婪，属于在线策略\n",
    "# 策略评估属于增量式更新\n",
    "# 动态调整ε：随着训练的进行，逐渐线性减小ε的值，以减少探索，增加利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy (Probabilities):\n",
      "State (0, 0): {'up': 0.025, 'down': 0.025, 'left': 0.025, 'right': 0.925}\n",
      "State (0, 1): {'up': 0.025, 'down': 0.025, 'left': 0.025, 'right': 0.925}\n",
      "State (0, 2): {'up': 0.025, 'down': 0.925, 'left': 0.025, 'right': 0.025}\n",
      "State (1, 0): {'up': 0.036250000000000004, 'down': 0.89125, 'left': 0.036250000000000004, 'right': 0.036250000000000004}\n",
      "State (1, 2): {'up': 0.025, 'down': 0.925, 'left': 0.025, 'right': 0.025}\n",
      "State (2, 0): {'up': 0.036250000000000004, 'down': 0.036250000000000004, 'left': 0.036250000000000004, 'right': 0.89125}\n",
      "State (2, 1): {'up': 0.036250000000000004, 'down': 0.036250000000000004, 'left': 0.036250000000000004, 'right': 0.89125}\n",
      "\n",
      "Action-Value Function:\n",
      "State ((2, 1), 'right'): -1.00\n",
      "State ((2, 1), 'up'): -5.52\n",
      "State ((2, 1), 'down'): -7.10\n",
      "State ((2, 0), 'right'): -3.87\n",
      "State ((2, 0), 'down'): -8.93\n",
      "State ((2, 0), 'left'): -10.53\n",
      "State ((1, 0), 'down'): -8.08\n",
      "State ((0, 0), 'down'): -11.05\n",
      "State ((0, 1), 'left'): -10.53\n",
      "State ((0, 1), 'up'): -10.21\n",
      "State ((0, 2), 'left'): -8.79\n",
      "State ((0, 2), 'right'): -6.31\n",
      "State ((0, 1), 'right'): -4.54\n",
      "State ((0, 1), 'down'): -9.14\n",
      "State ((0, 0), 'right'): -6.73\n",
      "State ((1, 2), 'up'): -7.78\n",
      "State ((0, 2), 'down'): -2.60\n",
      "State ((1, 0), 'up'): -14.36\n",
      "State ((0, 0), 'left'): -12.66\n",
      "State ((0, 0), 'up'): -11.38\n",
      "State ((0, 2), 'up'): -7.04\n",
      "State ((1, 2), 'down'): -1.00\n",
      "State ((1, 2), 'left'): -4.74\n",
      "State ((1, 2), 'right'): -4.29\n",
      "State ((2, 1), 'left'): -10.38\n",
      "State ((2, 0), 'up'): -13.59\n",
      "State ((1, 0), 'left'): -12.39\n",
      "State ((1, 0), 'right'): -12.84\n"
     ]
    }
   ],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon, actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(actions)\n",
    "    else:\n",
    "        return max(actions, key=lambda a: Q[(state, a)])\n",
    "\n",
    "def on_policy_monte_carlo_control(env, episodes=1000, gamma=1.0, epsilon_start=1.0, epsilon_min=0.1, decay_rate=0.001):\n",
    "    Q = defaultdict(float)\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(int)\n",
    "    \n",
    "    policy = defaultdict(lambda: env.actions[0])  # 初始化为固定动作\n",
    "    \n",
    "    for episode_num in range(episodes):\n",
    "        # 动态调整 ε\n",
    "        epsilon = max(epsilon_min, epsilon_start - decay_rate * episode_num)\n",
    "        \n",
    "        # 生成一条轨迹\n",
    "        state = env.start_state\n",
    "        episode = []\n",
    "        \n",
    "        while not env.is_terminal(state):\n",
    "            action = epsilon_greedy_policy(state, Q, epsilon, env.actions)\n",
    "            next_state, reward = env.step(state, action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        \n",
    "        # 计算每个状态-动作对的回报\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = reward + gamma * G\n",
    "            if (state, action) not in visited:\n",
    "                returns_sum[(state, action)] += G\n",
    "                returns_count[(state, action)] += 1\n",
    "                Q[(state, action)] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
    "                visited.add((state, action))\n",
    "        \n",
    "        # 策略改进: 使用 epsilon-greedy 概率更新策略\n",
    "        for state in set(s for s, _, _ in episode):\n",
    "            best_action = max(env.actions, key=lambda a: Q[(state, a)])\n",
    "            policy[state] = {}\n",
    "            for action in env.actions:\n",
    "                if action == best_action:\n",
    "                    policy[state][action] = 1 - epsilon + epsilon / len(env.actions)\n",
    "                else:\n",
    "                    policy[state][action] = epsilon / len(env.actions)\n",
    "    \n",
    "    # 返回最终策略及 Q 函数\n",
    "    return policy, Q\n",
    "\n",
    "env = MazeEnv()\n",
    "policy, Q = on_policy_monte_carlo_control(env, episodes=1000, epsilon_start=1.0, epsilon_min=0.1, decay_rate=0.001)\n",
    "\n",
    "print(\"Learned Policy (Probabilities):\")\n",
    "for state in sorted(policy):\n",
    "    print(f\"State {state}: {policy[state]}\")\n",
    "\n",
    "print(\"\\nAction-Value Function:\")\n",
    "for state_action, value in Q.items():\n",
    "    print(f\"State {state_action}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 行为策略和目标策略一样，都是ε-贪婪，属于在线策略\n",
    "# 策略评估属于增量式更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy (Probabilities):\n",
      "State (0, 0): {'up': 0.025, 'down': 0.025, 'left': 0.025, 'right': 0.925}\n",
      "State (0, 1): {'up': 0.025, 'down': 0.025, 'left': 0.025, 'right': 0.925}\n",
      "State (0, 2): {'up': 0.025, 'down': 0.925, 'left': 0.025, 'right': 0.025}\n",
      "State (1, 0): {'up': 0.025, 'down': 0.025, 'left': 0.925, 'right': 0.025}\n",
      "State (1, 2): {'up': 0.025, 'down': 0.925, 'left': 0.025, 'right': 0.025}\n",
      "State (2, 0): {'up': 0.025, 'down': 0.025, 'left': 0.025, 'right': 0.925}\n",
      "State (2, 1): {'up': 0.025, 'down': 0.025, 'left': 0.025, 'right': 0.925}\n",
      "\n",
      "Action-Value Function:\n",
      "State ((0, 0), 'up'): -22.05\n",
      "State ((0, 0), 'down'): -48.00\n",
      "State ((0, 0), 'left'): -24.29\n",
      "State ((0, 0), 'right'): -7.02\n",
      "State ((1, 0), 'up'): -26.87\n",
      "State ((1, 0), 'down'): -17.73\n",
      "State ((1, 0), 'left'): -6.69\n",
      "State ((1, 0), 'right'): -31.72\n",
      "State ((0, 1), 'up'): -16.12\n",
      "State ((0, 1), 'down'): -55.04\n",
      "State ((0, 1), 'left'): -26.70\n",
      "State ((0, 1), 'right'): -4.32\n",
      "State ((0, 2), 'up'): -3.09\n",
      "State ((0, 2), 'down'): -2.11\n",
      "State ((0, 2), 'left'): -19.65\n",
      "State ((0, 2), 'right'): -22.94\n",
      "State ((1, 2), 'up'): -6.31\n",
      "State ((1, 2), 'down'): -1.00\n",
      "State ((1, 2), 'left'): -2.06\n",
      "State ((1, 2), 'right'): -20.19\n",
      "State ((2, 0), 'up'): -28.67\n",
      "State ((2, 0), 'down'): -6.00\n",
      "State ((2, 0), 'left'): -97.00\n",
      "State ((2, 0), 'right'): -2.50\n",
      "State ((2, 1), 'right'): -1.00\n",
      "State ((2, 1), 'up'): -5.00\n",
      "State ((2, 1), 'down'): -2.00\n",
      "State ((2, 1), 'left'): -21.50\n"
     ]
    }
   ],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon, actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(actions)\n",
    "    else:\n",
    "        return max(actions, key=lambda a: Q[(state, a)])\n",
    "\n",
    "def on_policy_monte_carlo_control(env, episodes=1000, gamma=1.0, epsilon=0.1):\n",
    "    Q = defaultdict(float)\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(int)\n",
    "    \n",
    "    policy = defaultdict(lambda: env.actions[0])  # 初始化为固定动作\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        # 生成一条轨迹\n",
    "        state = env.start_state\n",
    "        episode = []\n",
    "        \n",
    "        while not env.is_terminal(state):\n",
    "            action = epsilon_greedy_policy(state, Q, epsilon, env.actions)\n",
    "            next_state, reward = env.step(state, action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        \n",
    "        # 计算每个状态-动作对的回报\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = reward + gamma * G\n",
    "            if (state, action) not in visited:\n",
    "                returns_sum[(state, action)] += G\n",
    "                returns_count[(state, action)] += 1\n",
    "                Q[(state, action)] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
    "                visited.add((state, action))\n",
    "        \n",
    "        # 策略改进: 使用 epsilon-greedy 概率更新策略\n",
    "        for state in set(s for s, _, _ in episode):\n",
    "            best_action = max(env.actions, key=lambda a: Q[(state, a)])\n",
    "            policy[state] = {}\n",
    "            for action in env.actions:\n",
    "                if action == best_action:\n",
    "                    policy[state][action] = 1 - epsilon + epsilon / len(env.actions)\n",
    "                else:\n",
    "                    policy[state][action] = epsilon / len(env.actions)\n",
    "    \n",
    "    # 返回最终策略及 Q 函数\n",
    "    return policy, Q\n",
    "\n",
    "env = MazeEnv()\n",
    "policy, Q = on_policy_monte_carlo_control(env, episodes=1000, epsilon=0.1)\n",
    "\n",
    "print(\"Learned Policy (Probabilities):\")\n",
    "for state in sorted(policy):\n",
    "    print(f\"State {state}: {policy[state]}\")\n",
    "\n",
    "print(\"\\nAction-Value Function:\")\n",
    "for state_action, value in Q.items():\n",
    "    print(f\"State {state_action}: {value:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
