{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Iterations:\n",
      "{0: 424, 1: 6}\n",
      "Optimal Value Function:\n",
      "[[-3. -2. -1.  0. -1.]\n",
      " [-2. -1.  0.  0.  0.]\n",
      " [-3. -2. -1.  0. -1.]\n",
      " [-4. -3. -2. -1. -2.]\n",
      " [-5. -4. -3. -2. -3.]]\n",
      "Optimal Policy:\n",
      "↓→ ↓→ ↓→ ↓ ↓←\n",
      "→ → → G ←\n",
      "↑→ ↑→ ↑→ ↑ ↑←\n",
      "↑→ ↑→ ↑→ ↑ ↑←\n",
      "↑→ ↑→ ↑→ ↑ ↑←\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# MDP 参数\n",
    "GRID_SIZE = 5\n",
    "ACTIONS = ['↑', '↓', '←', '→']  # 上、下、左、右\n",
    "DISCOUNT_FACTOR = 1.0\n",
    "THRESHOLD = 1e-4\n",
    "GOAL_STATE = (1, 3)\n",
    "\n",
    "def transition(state, action):\n",
    "    i, j = state\n",
    "    if action == '↑':\n",
    "        return max(i - 1, 0), j\n",
    "    elif action == '↓':\n",
    "        return min(i + 1, GRID_SIZE - 1), j\n",
    "    elif action == '←':\n",
    "        return i, max(j - 1, 0)\n",
    "    elif action == '→':\n",
    "        return i, min(j + 1, GRID_SIZE - 1)\n",
    "\n",
    "def reward_function(state):\n",
    "    if state == GOAL_STATE:\n",
    "        return 0  # 目标状态\n",
    "    return -1  # 其他状态的奖励\n",
    "\n",
    "def policy_evaluation(policy, V):\n",
    "    # 初始化迭代次数\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = np.zeros_like(V)\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                state = (i, j)\n",
    "                if state == GOAL_STATE:  # 跳过目标状态\n",
    "                    continue\n",
    "                action_values = []\n",
    "                for action in policy[state]:\n",
    "                    next_state = transition(state, action)\n",
    "                    reward = reward_function(next_state)\n",
    "                    action_values.append(reward + DISCOUNT_FACTOR * V[next_state])\n",
    "                new_V[state] = np.mean(action_values)  # 策略下的期望值，如果改为 max 则为贪婪策略，相当于值迭代\n",
    "                delta = max(delta, abs(new_V[state] - V[state]))\n",
    "        V = new_V\n",
    "        \n",
    "        if delta < THRESHOLD:\n",
    "            break\n",
    "        iteration += 1\n",
    "    return V, iteration\n",
    "\n",
    "def policy_iteration():\n",
    "    # 初始化策略和价值函数\n",
    "    policy = np.empty((GRID_SIZE, GRID_SIZE), dtype=object) # 创建一个 GRID_SIZE x GRID_SIZE 的数组，每个元素都是 ACTIONS 的副本\n",
    "    for i in range(GRID_SIZE):\n",
    "        for j in range(GRID_SIZE):\n",
    "            policy[i, j] = ACTIONS  # 其他状态\n",
    "    V = np.zeros((GRID_SIZE, GRID_SIZE))  # 初始化价值函数\n",
    "\n",
    "    iterationDict = {}\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        # 策略评估\n",
    "        V, eval_iteration = policy_evaluation(policy, V)\n",
    "        if eval_iteration == 0: # 如果策略评估没有进行任何迭代，说明策略已经收敛，没必要再进行策略改进\n",
    "            break\n",
    "        iterationDict[iteration] = eval_iteration\n",
    "\n",
    "        # 策略改进\n",
    "        policy_stable = True\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                state = (i, j)\n",
    "                if state == GOAL_STATE:  # 跳过目标状态\n",
    "                    policy[state] = ['G']\n",
    "                    continue\n",
    "                action_values = {}\n",
    "                for action in ACTIONS:\n",
    "                    next_state = transition(state, action)\n",
    "                    reward = reward_function(next_state)\n",
    "                    action_values[action] = reward + DISCOUNT_FACTOR * V[next_state]\n",
    "\n",
    "                best_actions = [action for action, value in action_values.items() if value == max(action_values.values())]\n",
    "\n",
    "                if set(policy[state]) != set(best_actions):\n",
    "                    policy_stable = False\n",
    "                    policy[state] = best_actions\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "        iteration += 1\n",
    "\n",
    "    return policy, V, iterationDict\n",
    "\n",
    "# 运行策略迭代\n",
    "optimal_policy, optimal_value, iterationDict = policy_iteration()\n",
    "\n",
    "# 打印迭代次数\n",
    "print(\"Number of Iterations:\")\n",
    "print(iterationDict)\n",
    "# 打印最优价值函数\n",
    "print(\"Optimal Value Function:\")\n",
    "print(optimal_value)\n",
    "# 打印最优策略\n",
    "print(\"Optimal Policy:\")\n",
    "for row in optimal_policy:\n",
    "    print(' '.join([''.join(a) for a in row]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Iterations:\n",
      "{0: 424, 1: 6}\n",
      "Optimal Value Function:\n",
      "[[-3. -2. -1.  0. -1.]\n",
      " [-2. -1.  0.  0.  0.]\n",
      " [-3. -2. -1.  0. -1.]\n",
      " [-4. -3. -2. -1. -2.]\n",
      " [-5. -4. -3. -2. -3.]]\n",
      "Optimal Policy:\n",
      "↓:0.50,→:0.50\t↓:0.50,→:0.50\t↓:0.50,→:0.50\t↓:1.00\t↓:0.50,←:0.50\t\n",
      "→:1.00\t→:1.00\t→:1.00\tG\t←:1.00\t\n",
      "↑:0.50,→:0.50\t↑:0.50,→:0.50\t↑:0.50,→:0.50\t↑:1.00\t↑:0.50,←:0.50\t\n",
      "↑:0.50,→:0.50\t↑:0.50,→:0.50\t↑:0.50,→:0.50\t↑:1.00\t↑:0.50,←:0.50\t\n",
      "↑:0.50,→:0.50\t↑:0.50,→:0.50\t↑:0.50,→:0.50\t↑:1.00\t↑:0.50,←:0.50\t\n"
     ]
    }
   ],
   "source": [
    "DISCOUNT_FACTOR = 1.0\n",
    "def policy_evaluation(policy, V):\n",
    "    # 初始化迭代次数\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = np.zeros_like(V)\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                state = (i, j)\n",
    "                if state == GOAL_STATE:  # 跳过目标状态\n",
    "                    continue\n",
    "                value = 0\n",
    "                for action, action_prob in policy[state].items():\n",
    "                    next_state = transition(state, action)\n",
    "                    reward = reward_function(next_state)\n",
    "                    value += action_prob * (reward + DISCOUNT_FACTOR * V[next_state])\n",
    "                new_V[state] = value\n",
    "                delta = max(delta, abs(new_V[state] - V[state]))\n",
    "        V = new_V\n",
    "        if delta < THRESHOLD:\n",
    "            break\n",
    "        iteration += 1\n",
    "    return V, iteration\n",
    "\n",
    "def policy_iteration():\n",
    "    # 初始随机策略，每个动作概率均等\n",
    "    policy = {\n",
    "        (i, j): {a: 1 / len(ACTIONS) for a in ACTIONS}\n",
    "        for i in range(GRID_SIZE)\n",
    "        for j in range(GRID_SIZE)\n",
    "    }\n",
    "    V = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "\n",
    "    iterationDict = {}\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        # 策略评估\n",
    "        V, eval_iteration = policy_evaluation(policy, V)\n",
    "        if eval_iteration == 0: # 如果策略评估没有进行任何迭代，说明策略已经收敛，没必要再进行策略改进\n",
    "            break\n",
    "        iterationDict[iteration] = eval_iteration\n",
    "\n",
    "        # 策略改进\n",
    "        policy_stable = True\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                state = (i, j)\n",
    "                if state == GOAL_STATE:\n",
    "                    continue\n",
    "\n",
    "                action_values = {}\n",
    "                for action in ACTIONS:\n",
    "                    next_state = transition(state, action)\n",
    "                    reward = reward_function(next_state)\n",
    "                    action_values[action] = reward + DISCOUNT_FACTOR * V[next_state]\n",
    "\n",
    "                max_value = max(action_values.values())\n",
    "                best_actions = [a for a in ACTIONS if action_values[a] == max_value]\n",
    "\n",
    "                # 更新策略：将最优动作平均分配概率\n",
    "                new_action_probs = {a: (1 / len(best_actions) if a in best_actions else 0) for a in ACTIONS}\n",
    "\n",
    "                if policy[state] != new_action_probs:\n",
    "                    policy_stable = False\n",
    "                policy[state] = new_action_probs\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "        iteration += 1\n",
    "\n",
    "    return policy, V, iterationDict\n",
    "\n",
    "# 运行策略迭代\n",
    "optimal_policy, optimal_value, iterationDict = policy_iteration()\n",
    "\n",
    "# 打印迭代次数\n",
    "print(\"Number of Iterations:\")\n",
    "print(iterationDict)\n",
    "# 打印最优价值函数\n",
    "print(\"Optimal Value Function:\")\n",
    "print(optimal_value)\n",
    "# 打印最优随机策略\n",
    "print(\"Optimal Policy:\")\n",
    "for i in range(GRID_SIZE):\n",
    "    for j in range(GRID_SIZE):\n",
    "        state = (i, j)\n",
    "        if state == GOAL_STATE:\n",
    "            print(\"G\", end=\"\\t\")\n",
    "        else:\n",
    "            actions = optimal_policy[state]\n",
    "            print(\",\".join([f\"{a}:{p:.2f}\" for a, p in actions.items() if p > 0]), end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Iterations:\n",
      "{0: 3, 1: 3, 2: 3}\n",
      "Optimal Value Function:\n",
      "[[-3. -2. -1.  0. -1.]\n",
      " [-2. -1.  0.  0.  0.]\n",
      " [-3. -2. -1.  0. -1.]\n",
      " [-4. -3. -2. -1. -2.]\n",
      " [-5. -4. -3. -2. -3.]]\n",
      "Optimal Policy:\n",
      "↓:0.50→:0.50\t↓:0.50→:0.50\t↓:0.50→:0.50\t↓:1.00\t↓:0.50←:0.50\t\n",
      "→:1.00\t→:1.00\t→:1.00\tG\t←:1.00\t\n",
      "↑:0.50→:0.50\t↑:0.50→:0.50\t↑:0.50→:0.50\t↑:1.00\t↑:0.50←:0.50\t\n",
      "↑:0.50→:0.50\t↑:0.50→:0.50\t↑:0.50→:0.50\t↑:1.00\t↑:0.50←:0.50\t\n",
      "↑:0.50→:0.50\t↑:0.50→:0.50\t↑:0.50→:0.50\t↑:1.00\t↑:0.50←:0.50\t\n"
     ]
    }
   ],
   "source": [
    "# 示例代码：广义策略迭代\n",
    "\n",
    "EVALUATION_STEPS = 3  # 每次策略评估只迭代有限步数\n",
    "\n",
    "def policy_evaluation(policy, V, steps=EVALUATION_STEPS):\n",
    "    # 初始化迭代次数\n",
    "    iteration = 0\n",
    "    for _ in range(steps):\n",
    "        delta = 0\n",
    "        new_V = np.zeros_like(V)\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                state = (i, j)\n",
    "                if state == GOAL_STATE:\n",
    "                    continue\n",
    "                value = 0\n",
    "                for action, action_prob in policy[state].items():\n",
    "                    next_state = transition(state, action)\n",
    "                    reward = reward_function(next_state)\n",
    "                    value += action_prob * (reward + DISCOUNT_FACTOR * V[next_state])\n",
    "                new_V[state] = value\n",
    "                delta = max(delta, abs(new_V[state] - V[state]))\n",
    "        V = new_V\n",
    "        if delta < THRESHOLD:\n",
    "            break\n",
    "        iteration += 1\n",
    "    return V, iteration\n",
    "\n",
    "def policy_improvement(V):\n",
    "    policy = {}\n",
    "    for i in range(GRID_SIZE):\n",
    "        for j in range(GRID_SIZE):\n",
    "            state = (i, j)\n",
    "            if state == GOAL_STATE:\n",
    "                continue\n",
    "            action_values = {}\n",
    "            for action in ACTIONS:\n",
    "                next_state = transition(state, action)\n",
    "                reward = reward_function(next_state)\n",
    "                action_values[action] = reward + DISCOUNT_FACTOR * V[next_state]\n",
    "            \n",
    "            max_value = max(action_values.values())\n",
    "            best_actions = [a for a in ACTIONS if action_values[a] == max_value]\n",
    "            \n",
    "            # 将最优动作均分概率\n",
    "            policy[state] = {a: 1 / len(best_actions) if a in best_actions else 0 for a in ACTIONS}\n",
    "    return policy\n",
    "\n",
    "def generalized_policy_iteration():\n",
    "    # 初始随机策略\n",
    "    policy = {\n",
    "        (i, j): {a: 1 / len(ACTIONS) for a in ACTIONS}\n",
    "        for i in range(GRID_SIZE)\n",
    "        for j in range(GRID_SIZE)\n",
    "    }\n",
    "    V = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "\n",
    "    iterationDict = {}\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        # 策略评估：只进行有限次迭代\n",
    "        V, eval_iteration = policy_evaluation(policy, V)\n",
    "        if eval_iteration == 0: # 如果策略评估没有进行任何迭代，说明策略已经收敛，没必要再进行策略改进\n",
    "            break\n",
    "        iterationDict[iteration] = eval_iteration\n",
    "\n",
    "        # 策略改进\n",
    "        new_policy = policy_improvement(V)\n",
    "\n",
    "        if new_policy == policy:\n",
    "            break\n",
    "        policy = new_policy\n",
    "        iteration += 1\n",
    "\n",
    "    return policy, V, iterationDict\n",
    "\n",
    "# 运行广义策略迭代\n",
    "optimal_policy, optimal_value, iterationDict = generalized_policy_iteration()\n",
    "\n",
    "# 打印迭代次数\n",
    "print(\"Number of Iterations:\")\n",
    "print(iterationDict)\n",
    "# 打印最优价值函数\n",
    "print(\"Optimal Value Function:\")\n",
    "print(optimal_value)\n",
    "# 打印最优策略\n",
    "print(\"Optimal Policy:\")\n",
    "for i in range(GRID_SIZE):\n",
    "    for j in range(GRID_SIZE):\n",
    "        state = (i, j)\n",
    "        if state == GOAL_STATE:\n",
    "            print(\"G\", end=\"\\t\")\n",
    "        else:\n",
    "            actions = optimal_policy[state]\n",
    "            print(\"\".join([f\"{a}:{p:.2f}\" for a, p in actions.items() if p > 0]), end=\"\\t\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
