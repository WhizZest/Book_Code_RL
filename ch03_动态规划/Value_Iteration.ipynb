{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Iterations: 7\n",
      "Optimal Value Function:\n",
      "[[ 7.  8.  9. 10.  9.]\n",
      " [ 8.  9. 10.  0. 10.]\n",
      " [ 7.  8.  9. 10.  9.]\n",
      " [ 6.  7.  8.  9.  8.]\n",
      " [ 5.  6.  7.  8.  7.]]\n",
      "\n",
      "Optimal Policy:\n",
      "['↓→' '↓→' '↓→' '↓' '↓←']\n",
      "['→' '→' '→' 'G' '←']\n",
      "['↑→' '↑→' '↑→' '↑' '↑←']\n",
      "['↑→' '↑→' '↑→' '↑' '↑←']\n",
      "['↑→' '↑→' '↑→' '↑' '↑←']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 网格世界的定义\n",
    "GRID_SIZE = 5  # 网格的大小为4x4\n",
    "ACTIONS = ['up', 'down', 'left', 'right']  # 可用的动作\n",
    "ACTION_TO_DELTA = {\n",
    "    'up': (-1, 0),    # 向上移动，行减1，列不变\n",
    "    'down': (1, 0),   # 向下移动，行加1，列不变\n",
    "    'left': (0, -1),  # 向左移动，行不变，列减1\n",
    "    'right': (0, 1)   # 向右移动，行不变，列加1\n",
    "}\n",
    "# 动作到箭头的映射\n",
    "ACTION_TO_ARROW = {\n",
    "    'up': '↑',\n",
    "    'down': '↓',\n",
    "    'left': '←',\n",
    "    'right': '→'\n",
    "}\n",
    "DISCOUNT_FACTOR = 1.0  # 折扣因子，用于计算未来奖励的折现值\n",
    "THRESHOLD = 1e-4  # 收敛阈值，用于判断值迭代是否收敛\n",
    "# 目标状态\n",
    "GOAL_STATE = (1, 3)\n",
    "\n",
    "# 奖励函数: 默认每次移动得到-1的奖励，到达目标状态得到 +10 的奖励\n",
    "def reward_function(state, goal_state=GOAL_STATE):\n",
    "    if state == goal_state:  # 目标状态\n",
    "        return 10\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# 转移函数：给定状态和动作，返回下一个状态和奖励\n",
    "def transition(state, action):\n",
    "    delta = ACTION_TO_DELTA[action]  # 获取动作对应的行和列的变化量\n",
    "    next_state = (state[0] + delta[0], state[1] + delta[1])  # 计算下一个状态\n",
    "    # 边界检测，确保在网格内\n",
    "    if 0 <= next_state[0] < GRID_SIZE and 0 <= next_state[1] < GRID_SIZE:\n",
    "        return next_state\n",
    "    else:  # 如果越界，返回原状态\n",
    "        return state\n",
    "\n",
    "# 值迭代算法\n",
    "def value_iteration(goal_state=GOAL_STATE):\n",
    "    V = np.zeros((GRID_SIZE, GRID_SIZE))  # 初始化价值函数，所有状态的价值初始为0\n",
    "    iteration_count = 0  # 初始化迭代次数\n",
    "    while True:\n",
    "        # 打印每次迭代的价值函数\n",
    "        #print(f\"Iteration {iteration_count}:\")\n",
    "        #print(V)\n",
    "        delta = 0  # 用于记录当前迭代中最大的价值变化\n",
    "        new_V = V.copy()  # 复制当前的价值函数用于更新\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                state = (i, j)\n",
    "                if state == goal_state:  # 目标状态无需更新\n",
    "                    continue\n",
    "                # 计算每个动作的价值\n",
    "                action_values = []\n",
    "                for action in ACTIONS:\n",
    "                    next_state = transition(state, action)  # 计算下一个状态\n",
    "                    reward = reward_function(next_state, goal_state)  # 计算奖励\n",
    "                    action_value = reward + DISCOUNT_FACTOR * V[next_state]  # 计算动作价值\n",
    "                    action_values.append(action_value)\n",
    "                # 更新当前状态的价值\n",
    "                new_V[state] = max(action_values)  # 选择最大动作价值作为当前状态的价值\n",
    "                delta = max(delta, abs(new_V[state] - V[state]))  # 更新最大价值变化\n",
    "        V = new_V  # 更新价值函数\n",
    "        iteration_count += 1  # 增加迭代次数\n",
    "        # 检查收敛\n",
    "        if delta < THRESHOLD:  # 如果最大价值变化小于收敛阈值，则认为收敛\n",
    "            break\n",
    "    return V, iteration_count  # 返回最优价值函数和迭代次数\n",
    "\n",
    "\n",
    "# 提取最优策略\n",
    "def extract_policy(V, goal_state=GOAL_STATE):\n",
    "    policy = np.empty((GRID_SIZE, GRID_SIZE), dtype=object)  # 初始化策略函数，使用 object 类型以存储多个动作\n",
    "    for i in range(GRID_SIZE):\n",
    "        for j in range(GRID_SIZE):\n",
    "            state = (i, j)\n",
    "            if state == goal_state:  # 目标状态\n",
    "                policy[state] = ''.join(['G'])  # 目标状态，策略为到达目标\n",
    "                continue\n",
    "            action_values = {}  # 存储每个动作的价值\n",
    "            for action in ACTIONS:\n",
    "                next_state = transition(state, action)  # 计算下一个状态\n",
    "                reward = reward_function(next_state, goal_state)  # 计算奖励\n",
    "                action_values[action] = reward + DISCOUNT_FACTOR * V[next_state]  # 计算动作价值\n",
    "            \n",
    "            # 找出所有最优动作（可能多个）\n",
    "            max_value = max(action_values.values())  # 找到最大动作价值\n",
    "            best_actions = [ACTION_TO_ARROW[action] for action, value in action_values.items() if value == max_value]  # 找到所有具有最大动作价值的动作\n",
    "            # 拼接最优动作为字符串\n",
    "            policy[state] = ''.join(best_actions)  # 将最优动作存储在策略中\n",
    "    return policy\n",
    "\n",
    "\n",
    "# 运行值迭代\n",
    "V, iteration_count = value_iteration(GOAL_STATE)\n",
    "optimal_policy = extract_policy(V, GOAL_STATE)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Number of Iterations:\", iteration_count)\n",
    "print(\"Optimal Value Function:\")\n",
    "print(np.round(V, 2))  # 打印最优价值函数，保留两位小数\n",
    "print(\"\\nOptimal Policy:\")\n",
    "# 打印最优策略\n",
    "for row in optimal_policy:\n",
    "    print(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
