{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . .\n",
      ". T T T .\n",
      ". . . . .\n",
      ". T . T .\n",
      "T . G . T\n",
      "\n",
      "Starting TD(0) training with epsilon-greedy policy...\n",
      "\n",
      "\n",
      "Learned State Values:\n",
      "[[ 0.00829893 -0.02243397 -0.07837071 -0.03839208  0.0017454 ]\n",
      " [ 0.12715385  0.          0.          0.          0.1303622 ]\n",
      " [ 0.36119756  0.3878087   0.62790981  0.40720198  0.31008431]\n",
      " [-0.02693164  0.          0.8112383   0.         -0.05218608]\n",
      " [ 0.          0.5217031   0.          0.468559    0.        ]]\n",
      "\n",
      "Optimal Policy:\n",
      "↓ ← ← → ↓\n",
      "↓ T T T ↓\n",
      "→ → ↓ ← ←\n",
      "↑ T ↓ T ↑\n",
      "T → G ← T\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self, grid_size=5, traps=[(1, 1), (1, 2), (1, 3), (3, 1), (3, 3), (4, 0), (4, 4)], goal=(4, 2)):\n",
    "        self.grid_size = grid_size  # 网格的大小，默认为5x5\n",
    "        self.goal = goal  # 目标位置，默认为(4, 4)\n",
    "        self.traps = traps  # 陷阱的位置列表\n",
    "        self.state_value = np.zeros((grid_size, grid_size))  # 状态价值函数，初始化为全零矩阵\n",
    "\n",
    "    def generate_traps(self, trapNum, seed=43):\n",
    "        trap_candidates = [\n",
    "            (i, j) for i in range(self.grid_size) for j in range(self.grid_size)\n",
    "            if (i, j) != self.goal  # 排除目标位置\n",
    "        ]\n",
    "        if seed is not None:\n",
    "            random.seed(seed)  # 设置随机种子，确保每次生成的陷阱位置相同\n",
    "        self.traps = random.sample(trap_candidates, trapNum)  # 从候选陷阱位置中随机选择指定数量的陷阱位置\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return state == self.goal or state in self.traps  # 判断状态是否为终止状态（到达目标或掉入陷阱）\n",
    "\n",
    "    def get_next_state(self, state, action):\n",
    "        x, y = state\n",
    "        if action == \"up\":\n",
    "            x = max(x - 1, 0)  # 向上移动，但不能超出网格边界\n",
    "        elif action == \"down\":\n",
    "            x = min(x + 1, self.grid_size - 1)  # 向下移动，但不能超出网格边界\n",
    "        elif action == \"left\":\n",
    "            y = max(y - 1, 0)  # 向左移动，但不能超出网格边界\n",
    "        elif action == \"right\":\n",
    "            y = min(y + 1, self.grid_size - 1)  # 向右移动，但不能超出网格边界\n",
    "        return (x, y)  # 返回新的状态\n",
    "\n",
    "    def get_reward(self, state):\n",
    "        if state == self.goal:\n",
    "            return 10  # 到达目标的奖励\n",
    "        elif state in self.traps:\n",
    "            return -10  # 掉入陷阱的惩罚\n",
    "        else:\n",
    "            return -1  # 普通状态的负奖励\n",
    "\n",
    "    def getRandomStartState(self):\n",
    "        \"\"\"随机选择一个非终止状态作为起点\"\"\"\n",
    "        possible_states = [\n",
    "            (i, j) for i in range(self.grid_size) for j in range(self.grid_size)\n",
    "            if (i, j) != self.goal and (i, j) not in self.traps\n",
    "        ]\n",
    "        return random.choice(possible_states)\n",
    "\n",
    "    def render(self):\n",
    "        grid = [[\".\" for _ in range(self.grid_size)] for _ in range(self.grid_size)]  # 创建一个网格，初始状态下所有位置都是\".\"\n",
    "        for trap in self.traps:\n",
    "            grid[trap[0]][trap[1]] = \"T\"  # 将陷阱位置标记为\"T\"\n",
    "        grid[self.goal[0]][self.goal[1]] = \"G\"  # 将目标位置标记为\"G\"\n",
    "        for row in grid:\n",
    "            print(\" \".join(row))  # 打印网格\n",
    "        print()\n",
    "\n",
    "\n",
    "def epsilon_greedy_action(env, state, epsilon, gamma=0.9):\n",
    "    \"\"\"选择动作，采用 epsilon-greedy 策略\"\"\"\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    if random.random() < epsilon:\n",
    "        # 探索：随机选择动作\n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        # 利用：选择即时奖励和未来价值最大的动作\n",
    "        values = {}\n",
    "        for action in actions:\n",
    "            next_state = env.get_next_state(state, action)\n",
    "            reward = env.get_reward(next_state)\n",
    "            values[action] = reward + gamma * env.state_value[next_state]\n",
    "        return max(values, key=values.get)\n",
    "\n",
    "# TD(0) 训练\n",
    "def td_training_with_policy(env, episodes=500, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    for episode in range(episodes):\n",
    "        state = env.getRandomStartState()\n",
    "        while not env.is_terminal(state):\n",
    "            action = epsilon_greedy_action(env, state, epsilon, gamma)\n",
    "            next_state = env.get_next_state(state, action)\n",
    "            reward = env.get_reward(next_state)\n",
    "            env.state_value[state] += alpha * (\n",
    "                reward + gamma * env.state_value[next_state] - env.state_value[state]\n",
    "            )\n",
    "            state = next_state\n",
    "        #if (episode + 1) % 100 == 0:\n",
    "            #print(f\"Episode {episode + 1}/{episodes} completed.\")\n",
    "\n",
    "def print_optimal_policy(env, gamma=0.9):\n",
    "    \"\"\"打印最优策略，考虑一个状态可能有多个最优动作\"\"\"\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    action_symbols = {\"up\": \"↑\", \"down\": \"↓\", \"left\": \"←\", \"right\": \"→\"}\n",
    "    policy = [[\".\" for _ in range(env.grid_size)] for _ in range(env.grid_size)]\n",
    "    \n",
    "    for i in range(env.grid_size):\n",
    "        for j in range(env.grid_size):\n",
    "            state = (i, j)\n",
    "            if env.is_terminal(state):\n",
    "                if state == env.goal:\n",
    "                    policy[i][j] = \"G\"\n",
    "                elif state in env.traps:\n",
    "                    policy[i][j] = \"T\"\n",
    "            else:\n",
    "                # 计算动作-状态值函数 Q(s, a)\n",
    "                q_values = {}\n",
    "                for action in actions:\n",
    "                    next_state = env.get_next_state(state, action)\n",
    "                    reward = env.get_reward(next_state)\n",
    "                    q_values[action] = reward + gamma * env.state_value[next_state]\n",
    "                \n",
    "                # 找到最大值对应的动作集合\n",
    "                max_value = max(q_values.values())\n",
    "                best_actions = [action_symbols[action] for action, value in q_values.items() if value == max_value]\n",
    "                \n",
    "                # 用多个符号表示最优动作\n",
    "                policy[i][j] = \"\".join(best_actions)\n",
    "    \n",
    "    print(\"\\nOptimal Policy:\")\n",
    "    for row in policy:\n",
    "        print(\" \".join(row))\n",
    "\n",
    "# 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "    env = Gridworld(grid_size=5, traps=[(1, 1), (1, 2), (1, 3), (3, 1), (3, 3), (4, 0), (4, 4)], goal=(4, 2))  # 创建环境\n",
    "    env.render()  # 显示网格\n",
    "    \n",
    "    print(\"Starting TD(0) training with epsilon-greedy policy...\\n\")\n",
    "    td_training_with_policy(env, episodes=100, alpha=0.1, gamma=0.9, epsilon=0.1)  # 训练\n",
    "    \n",
    "    print(\"\\nLearned State Values:\")\n",
    "    print(env.state_value)\n",
    "    \n",
    "    print_optimal_policy(env)  # 输出最优策略\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . .\n",
      ". T T T .\n",
      ". . . . .\n",
      ". T . T .\n",
      "T . G . T\n",
      "\n",
      "\n",
      "Optimal Policy:\n",
      "↓ ← ← ← ↓\n",
      "↓ T T T ↓\n",
      "→ → ↓ ← ←\n",
      "↑ T ↓ T ↑\n",
      "T → G ← T\n"
     ]
    }
   ],
   "source": [
    "def epsilon_greedy_action(state, Q, actions, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(actions)  # 随机选择动作\n",
    "    else:\n",
    "        # 选择具有最大Q值的动作\n",
    "        max_value = max(Q[state][a] for a in actions)\n",
    "        best_actions = [a for a in actions if Q[state][a] == max_value]\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "\n",
    "def sarsa(env, episodes=500, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    \"\"\"SARSA 算法\"\"\"\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    Q = {  # 初始化 Q 表\n",
    "        (i, j): {a: 0 for a in actions}\n",
    "        for i in range(env.grid_size)\n",
    "        for j in range(env.grid_size)\n",
    "    }\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.getRandomStartState()  # 初始化起始状态\n",
    "        action = epsilon_greedy_action(state, Q, actions, epsilon)  # 选择初始动作\n",
    "\n",
    "        while not env.is_terminal(state):\n",
    "            next_state = env.get_next_state(state, action)  # 执行动作，得到下一个状态\n",
    "            reward = env.get_reward(next_state)  # 即时奖励\n",
    "            next_action = epsilon_greedy_action(next_state, Q, actions, epsilon)  # 选择下一个动作\n",
    "\n",
    "            # SARSA 更新公式\n",
    "            Q[state][action] += alpha * (\n",
    "                reward + gamma * Q[next_state][next_action] - Q[state][action]\n",
    "            )\n",
    "\n",
    "            # 状态和动作更新\n",
    "            state, action = next_state, next_action\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n",
    "def print_optimal_policy(env, Q):\n",
    "    \"\"\"打印最优策略\"\"\"\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    action_symbols = {\"up\": \"↑\", \"down\": \"↓\", \"left\": \"←\", \"right\": \"→\"}\n",
    "    policy = [[\".\" for _ in range(env.grid_size)] for _ in range(env.grid_size)]\n",
    "\n",
    "    for i in range(env.grid_size):\n",
    "        for j in range(env.grid_size):\n",
    "            state = (i, j)\n",
    "            if env.is_terminal(state):\n",
    "                if state == env.goal:\n",
    "                    policy[i][j] = \"G\"\n",
    "                elif state in env.traps:\n",
    "                    policy[i][j] = \"T\"\n",
    "            else:\n",
    "                # 找到Q值最大的动作\n",
    "                max_value = max(Q[state][a] for a in actions)\n",
    "                best_actions = [action_symbols[a] for a in actions if Q[state][a] == max_value]\n",
    "                policy[i][j] = \"\".join(best_actions)\n",
    "\n",
    "    print(\"\\nOptimal Policy:\")\n",
    "    for row in policy:\n",
    "        print(\" \".join(row))\n",
    "\n",
    "\n",
    "# 创建 Gridworld 环境\n",
    "env = Gridworld(grid_size=5, traps=[(1, 1), (1, 2), (1, 3), (3, 1), (3, 3), (4, 0), (4, 4)], goal=(4, 2))\n",
    "env.render()\n",
    "\n",
    "# 使用 SARSA 训练\n",
    "Q = sarsa(env, episodes=100, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
    "\n",
    "# 打印最优策略\n",
    "print_optimal_policy(env, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . .\n",
      ". T T T .\n",
      ". . . . .\n",
      ". T . T .\n",
      "T . G . T\n",
      "\n",
      "\n",
      "Optimal Policy:\n",
      "↓ ← ← ← ↓\n",
      "↓ T T T ↓\n",
      "→ → ↓ ← ←\n",
      "↑ T ↓ T ↑\n",
      "T → G ← T\n"
     ]
    }
   ],
   "source": [
    "def q_learning(env, episodes=500, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    \"\"\"Q-Learning 算法\"\"\"\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    Q = {  # 初始化 Q 表\n",
    "        (i, j): {a: 0 for a in actions}\n",
    "        for i in range(env.grid_size)\n",
    "        for j in range(env.grid_size)\n",
    "    }\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.getRandomStartState()  # 初始化起始状态\n",
    "\n",
    "        while not env.is_terminal(state):\n",
    "            action = epsilon_greedy_action(state, Q, actions, epsilon)  # 选择动作\n",
    "            next_state = env.get_next_state(state, action)  # 执行动作，得到下一个状态\n",
    "            reward = env.get_reward(next_state)  # 即时奖励\n",
    "\n",
    "            # Q-Learning 更新公式\n",
    "            max_next_q = max(Q[next_state][a] for a in actions)\n",
    "            Q[state][action] += alpha * (\n",
    "                reward + gamma * max_next_q - Q[state][action]\n",
    "            )\n",
    "\n",
    "            state = next_state  # 更新状态\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n",
    "def print_optimal_policy(env, Q):\n",
    "    \"\"\"打印最优策略\"\"\"\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    action_symbols = {\"up\": \"↑\", \"down\": \"↓\", \"left\": \"←\", \"right\": \"→\"}\n",
    "    policy = [[\".\" for _ in range(env.grid_size)] for _ in range(env.grid_size)]\n",
    "\n",
    "    for i in range(env.grid_size):\n",
    "        for j in range(env.grid_size):\n",
    "            state = (i, j)\n",
    "            if env.is_terminal(state):\n",
    "                if state == env.goal:\n",
    "                    policy[i][j] = \"G\"\n",
    "                elif state in env.traps:\n",
    "                    policy[i][j] = \"T\"\n",
    "            else:\n",
    "                # 找到 Q 值最大的动作\n",
    "                max_value = max(Q[state][a] for a in actions)\n",
    "                best_actions = [action_symbols[a] for a in actions if Q[state][a] == max_value]\n",
    "                policy[i][j] = \"\".join(best_actions)\n",
    "\n",
    "    print(\"\\nOptimal Policy:\")\n",
    "    for row in policy:\n",
    "        print(\" \".join(row))\n",
    "\n",
    "\n",
    "# 创建 Gridworld 环境\n",
    "env = Gridworld(grid_size=5, traps=[(1, 1), (1, 2), (1, 3), (3, 1), (3, 3), (4, 0), (4, 4)], goal=(4, 2))\n",
    "env.render()\n",
    "\n",
    "# 使用 Q-Learning 训练\n",
    "Q = q_learning(env, episodes=200, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
    "\n",
    "# 打印最优策略\n",
    "print_optimal_policy(env, Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
